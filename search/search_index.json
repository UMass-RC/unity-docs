{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction to Unity","text":"<p>Welcome to Unity! The Unity cluster is a collaborative, multi-institutional high-performance computing cluster located at the Massachusetts Green High Performance Computing Center (MGHPCC). The cluster is under active development and supports primarily research activities. Partnering institutions currently include UMass Amherst, UMass Dartmouth, and University of Rhode Island.</p>"},{"location":"index.html#accessing-unity","title":"Accessing Unity","text":"<p>Faculty/PI's: If you are a Faculty member or Principal Investigator (PI), please create an account on the Unity main page. Use your campus NetID and password to sign up for a Unity account.  Once your account is created, go to Account Settings and request PI status by clicking the \"Request PI account\" button.  Once your account is approved you will be able to approve adding additional users to your PI group in the \u201cMy Users\u201d button on the side menu.  Adding users will grant them access to your PI <code>/work</code> directory.  Users in your group need to request access to join your PI group.  You cannot create accounts for your users.</p> <p>Students: In order to use Unity you must be working with a PI and they must grant you access to their PI group.</p> <p>Create an account on the Unity main page. Click \"Login/Request Account\". Click on your institution or search for it via the search bar. Once your account is created you will need to assign a PI to it.  To do this, go to \"My PIs\" using the left side menu and click the large \"+\" button. Follow the prompts to generate your unique SSH key. You can search for your PI by name. Once found, click on \u201cSend Request\u201d. After your PI approves your request, you will have access to Unity and your PI\u2019s project code and data stored in Unity. You can view, join, or leave a PI group by clicking \"My PIs\".</p> <p>Note: If you are a student, do not request a PI account.  This will slow the process down.  You should assign a PI to your account by clicking the \"+\" button.  Your PI will then be able to approve your account.</p> <p>Something not working for you?  Send an email to hpc@umass.edu with as much detail as you can provide to open a support ticket. Need additional help?  We offer office hours every week on Tuesdays 2:30-4 PM on Zoom. Be sure to check the cluster notes page for up-to-date information on any canceled/delayed office hours.</p> <p>Need expert help using the cluster and optimizing your code?  We encourage you to schedule an appointment with one of our experienced HPC facilitators.  Send an email to hpc@umass.edu and request an HPC facilitator consultation.</p>"},{"location":"index.html#connecting-to-the-cluster","title":"Connecting to the cluster","text":"<p>You can connect to Unity in two ways, an SSH connection (the standard Linux console), or an instance of JupyterHub:</p> <p>JupyterHub is the easiest to get up and going. To work with JupyterHub, it's a good idea to get acquainted with roughly how demanding the job you're uploading is though. You just need to be familiar with how roughly how many of each resource you will need (Image below). Most of the time you will only ever need a single CPU or GPU, but if you have terabytes of data to analyze, then you should probably consider getting multiple GPUs and CPUs.</p> <p>When connecting the portal, click on JupyterHub tab located at the bottom of the options list on the left side of the window. This will take you to the JupyterHub for Unity, which looks like this:</p> <p></p> <p>You will be asked to select what computer resources you want/need for the job you want to upload. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterHub as if it is running on your own computer.</p> <p>SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible for starting your own jobs. This can be more useful than JupyterHub's for jobs that last a long time and must be left unattended, or to have much more refined control over the resources allocated for your job.</p>"},{"location":"index.html#requesting-resources","title":"Requesting Resources","text":"<p>If you are on an SSH connection, you will have to request resources with Slurm. Once you decide on what resources you want, you will submit that information to the scheduler, which will place your job in the queue. When the resources are available, your job will start.</p> <p>Requesting resources in the cluster and all parameters allowed is discussed in more detail here.</p>"},{"location":"index.html#starting-job","title":"Starting Job","text":"<p>Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client.</p>"},{"location":"index.html#ending-job","title":"Ending Job","text":"<p>Once the job has finished, the scheduler will return whatever info you requested in your parameters.</p>"},{"location":"index.html#how-the-unity-cluster-works","title":"How the Unity Cluster Works","text":"<p>It's recommended that you read the \"HPC Jargon\" section of the common terms page.</p>"},{"location":"index.html#the-basics","title":"The Basics","text":"<p>Unity is a High Performance Computing Cluster (HPC Cluster). That is, a system made up of many smaller computers (nodes) that work together to behave like one large computer. An HPC cluster focuses most on computational power and efficiency, as the name entails. HPC allows a user to utilize the power of many computers simultaneously. This is useful for large tasks that are infeasible for a common laptop or desktop.</p> <p>When you use your personal computer, the operating system decides what resources (CPU, RAM, etc.) should be used for a given task. For example, if you were to open a calculator program and multiply some numbers, the operating system would determine what CPU threads should be used to perform the calculation, and how much of the graphics card should be used in rendering the calculator on your screen.</p> <p>An HPC job scheduler does the same thing but on a much bigger scale. Instead of a single process on one machine, an HPC cluster handles jobs which can be run on many machines at one time. With multiple users, the scheduler must manage incoming requests, reserve the resources, and connect those resources to those who have requested them. The Unity job scheduler is called Slurm.</p>"},{"location":"index.html#how-unity-works","title":"How Unity Works","text":"<p>Here is a general step by step process that governs how the Unity Cluster works:</p> <p></p> <ol> <li>Client connects to Unity using SSH or JupyterHub.</li> <li>Client schedules a job through Slurm.</li> <li>Slurm allocates resources and connects the client to them.</li> <li>Job runs.<ul> <li>Interactive job: Client does their work until they close the session or run out of time.</li> <li>Batch job: The job runs until it finishes its tasks or runs out of time.</li> </ul> </li> <li>Job returns output.<ul> <li>Batch job: The output to the command line is saved by slurm to a file like <code>slurm-&lt;JobID&gt;.out</code>.</li> </ul> </li> </ol>"},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#how-do-i-connect-to-and-start-using-the-cluster","title":"How do I connect to, and start using the cluster?","text":"<p>Refer to connection instructions on connecting here. You can connect with Putty, SSH in your terminal, or JupyterHub in your browser.</p>"},{"location":"faq.html#when-i-connect-over-ssh-i-get-a-message-saying-permission-denied-public-key","title":"When I connect over SSH I get a message saying <code>permission denied (public key)</code>","text":"<p>This can be due one of these common reasons:</p> <ul> <li>You have not provided your private key while connecting. <code>ssh -i &lt;private_key_location&gt; &lt;user&gt;@unity.rc.umass.edu</code></li> <li>You are not assigned to at least one PI group. We require at least one PI to endorse your account before you can use the cluster. Request to join a PI on the My PIs page.</li> <li>You have not added a public key to your account on Unity yet. You can do this on the Account Settings page.</li> <li>Your login shell is invalid. In Account Settings, try \"/bin/bash\" or \"/bin/zsh\".</li> <li>You are a PI, and you are trying to use your PI group name to log in. Your login username should not start with <code>pi_</code>.</li> </ul>"},{"location":"faq.html#where-can-i-find-software-to-use-on-the-cluster","title":"Where can I find software to use on the cluster?","text":"<p>Most of our software is package installed and is available by default. Non standard and version specific software are available as modules. The command <code>module av</code> will print all available modules. <code>module av &lt;name&gt;</code> will filter the available modules. Then you can use <code>module load &lt;name&gt;</code> to load a module and have access to its binaries (executables).</p>"},{"location":"faq.html#im-looking-for-xyz-software-could-you-install-it","title":"I'm looking for xyz software, could you install it?","text":"<p>Most software that is requested is free for use. If this is the case we will install it for you, just send us an email at hpc@umass.edu titled \"software request: \\&lt;name&gt;\". If the software you want is licensed, we may be able to help since the campus often has site-wide licenses for many applications.</p>"},{"location":"faq.html#can-i-run-containers-on-unity","title":"Can I run containers on Unity?","text":"<p>Yes! We support singularity containers, which are fully compatible with docker images. Run \"module load singularity\" to access it.</p>"},{"location":"faq.html#how-much-storage-do-i-get-on-unity-and-is-it-backed-up","title":"How much storage do I get on Unity and is it backed up?","text":"<p>Refer to storage information here. We do not provide backup solutions by default. We take snapshots of <code>/home/</code> and <code>/work/</code> every day at 1AM, but delete them after two days.</p>"},{"location":"faq.html#when-i-try-to-queue-a-job-i-get-denied-for-maxcpuperaccount","title":"When I try to queue a job I get denied for MaxCpuPerAccount.","text":"<p>Resource limits are set per lab. Currently, they are 300 CPUs, and 64 GPUs. This allocation is shared across your entire PI group.</p>"},{"location":"faq.html#im-a-pi-and-i-would-like-to-purchase-hardware-to-buy-in-to-unity","title":"I'm a PI and I would like to purchase hardware to buy-in to Unity.","text":"<p>Great! Send us an email and we'll be happy to help. We are very flexible when it comes to the needs of research labs.</p>"},{"location":"jargon.html","title":"Common Terms","text":"<p>Here are some definitions for common terms used in this documentation:</p>"},{"location":"jargon.html#hpc-jargon","title":"HPC Jargon","text":"<p>High Performance Computer Cluster (HPC Cluster): A system made up of many smaller computers that behaves like one large computer. HPC allows a user to utilize the power of many computers simultaneously.</p> <p>Scheduler: Distributor of resources. If you request 2 CPU cores for your job, the scheduler will reserve those cores and connect you to them. In a multi-threaded computer, the operating system also distributes resources and manages tasks like a job scheduler. The Scheduler for the Unity cluster is called Slurm.</p> <p>Node: One of the computers that serves as part of an HPC cluster. See node list.</p> <ul> <li>Login Node: This is what you are running on when you first log in to Unity via ssh. Only small tasks should be run here. The login nodes have strict CPU and memory limits to disincentivize running big tasks.<ul> <li>Also known as a Head Node in other clusters.</li> </ul> </li> <li>Compute Node: These are nodes intended for your big tasks. These are accessed by scheduling jobs in Slurm.</li> </ul> <p>Job: The task that you provide for the cluster to execute, and what resources should be allocated to do so.</p> <p>Partition: A grouping of nodes. See partition list.</p> <p>Queue: Often used interchangeably with partition.</p> <p></p> <p>Principal Investigator (PI): The individual responsible for a research grant. All users on Unity must be tied to a PI. Learn more about PI's</p>"},{"location":"jargon.html#software-jargon","title":"Software Jargon","text":"<p>Package: Installed software. Includes binaries, libraries, and other files. Typically cannot be moved about the filesystem once installed.</p> <p>Binary: An executable file.</p> <p>Library: Allows distribution of C code about the filesystem. Learn more about libraries</p> <p>Module: Script that modifies your environment to include a package. See environment modules.</p> <p>Environment: Set of environment variables. Defines the state of your current login shell. See environment modules.</p> <p>Conda environment: Set of installed packages. Activating a conda environment modifies your shell environment to include these packages. See conda.</p> <p>Compile: A compiler takes source code and turns it into binaries and libraries. Source code alone cannot be run by an operating system, it must be compiled into machine code or interpreted by an interpreter. C is compiled, Python is interpreted (sort of).</p> <ul> <li>Learn more about compilers</li> <li>Compiled vs Interpreted Languages</li> </ul>"},{"location":"jargon.html#linux-jargon","title":"Linux Jargon","text":"<p>Interface: The connection between you and your program.</p> <ul> <li>Graphical User Interface (GUI): The general term for graphical applications, where you click buttons, type in text boxes, and see visuals. Anything that uses the mouse.</li> <li>Command Line Interface (CLI): The general term for text-based applications, where you enter commands and then read whatever comes out. Uses only the keyboard.</li> </ul> <p>Console: Often used interchangeably with terminal.</p> <p>Terminal: The terminal is the graphical application that you type your commands in. In windows, this can be the the Command Prompt (<code>cmd</code>) or the Windows Terminal App (recommended). In Mac, it's just called Terminal. The terminal runs the shell.</p> <ul> <li>You can think of the terminal as just the borders of the window where you type commands. You can think of the shell as what's inside those borders.</li> </ul> <p>Shell: The shell interprets your commands and starts processes on the operating system. It depends on the terminal to take input from the user and to display output to the user. The shell stores your environment and allows you to write shell scripts. Learn more about shells</p>"},{"location":"jargon.html#linux-filesystem-jargon","title":"Linux Filesystem Jargon","text":"<p>Symlink: Shortcut.</p> <p>Directory: Folder.</p> <ul> <li> <p>Current Working Directory (CWD): The directory that is currently open in your program (often the shell). In a path, the CWD can be referenced with the <code>.</code> symbol.</p> </li> <li> <p>Parent Directory: The parent is the directory that is one above this directory. Every directory has a parent, except the root. In a path, the parent directory can be referenced with the <code>..</code> symbol.</p> </li> </ul> <p>Path: A path is a list of directories delimited by <code>/</code> slashes, followed by the name of a file. If the filesystem were a city, a path would be the directions to your apartment. Each directory in the path would be which road to take at the next crossroads.</p> <ul> <li>Relative Path: A path that does not start from a known landmark but from your current location <code>CWD</code>.<ul> <li><code>./.conda/envs/testName/bin/activate</code> is a relative path.</li> </ul> </li> <li>Absolute Path: A path that starts from a known landmark root <code>/</code>.<ul> <li><code>/home/$USER/.conda/envs/testName/bin/activate</code> is an absolute path.</li> </ul> </li> </ul> <p>Root: The filesystem can be visualized like a tree (not the plant, the data structure). The root of the tree is the directory from which all others descend from.</p>"},{"location":"roadmap.html","title":"Unity Cluster Roadmap","text":"<p>Unity is in Beta! With the addition of additional service hardware and storage devices, we are more confident in the stability of the cluster, and thus are ready to call it in \"beta\" state. In beta phase expect planned downtime more frequently than production research clusters, but you can expect the downtime periods to be shorter and more informative.</p>"},{"location":"roadmap.html#general","title":"General","text":"<ul> <li>Wider support for common applications</li> <li>Support for singularity containers within Slurm</li> <li>VNC sessions from within jupyterhub for running GUI applications</li> <li>Infiniband support for inter-node MPI</li> <li>Graphical usage report</li> <li>Support for multiple PIs attached to one user</li> <li>Support for dynamic creation of shared drives</li> </ul>"},{"location":"roadmap.html#website","title":"Website","text":"<ul> <li>Finish writing user documentation.</li> <li>Add content to the landing page(s)</li> </ul>"},{"location":"roadmap.html#hardware","title":"Hardware","text":"<ul> <li>Multiple service nodes for reliability</li> <li>High-speed flash storage node for scratch space</li> <li>600+ TB spinning disk NAS for general storage</li> <li>32 GPU nodes with 8 2080TIs each inside</li> </ul>"},{"location":"transition.html","title":"Transitioning from the Shared Cluster to Unity","text":"<p>Last updated: 21 April 2023</p> <p>It is our pleasure to welcome you to Unity, a high performance computing cluster located at MGHPCC and managed by UMass Amherst Research Computing. We understand that migrating your work may be stressful and we want to make it as smooth as possible. Please see below for important dates and information to guide your transition to Unity.</p> <p>For questions about the Unity transition, please attend our onboarding sessions or contact the Unity help desk at hpc@umass.edu.</p> <p>In addition, we have created a new Unity User Community. Please sign up with your school email here. If you\u2019re unable to register with your school email, please contact hpc@umass.edu with your preferred email address and we\u2019ll send you a direct invite.</p> <p>Once you\u2019ve signed up, you can access the community here.</p> <ul> <li>Transitioning from the Shared Cluster to Unity<ul> <li>Account Information<ul> <li>PI &amp; Faculty Accounts</li> <li>Student &amp; Postdoc Accounts</li> </ul> </li> <li>Timeline Overview<ul> <li>Migrating Data to Unity (deadline: May 15, 2023)</li> <li>Shared Cluster Shutdown (read-only: May 1, 2023, final shutdown: June 1, 2023)</li> </ul> </li> <li>Onboarding Live Sessions<ul> <li>Unity Onboarding Workshop Part 1: Access and Data Storage (March 31, 2023)</li> <li>Unity Onboarding Workshop Part 2: Running Jobs (April 14, 2023)</li> </ul> </li> <li>Technical Information<ul> <li>Data Transfer</li> <li>Job Scheduler</li> </ul> </li> </ul> </li> </ul>"},{"location":"transition.html#account-information","title":"Account Information","text":""},{"location":"transition.html#pi-faculty-accounts","title":"PI &amp; Faculty Accounts","text":"<p>PIs and faculty can request Unity cluster PI accounts in the Unity Portal. On the left side bar, select \u201cLogin / Request Account\u201d and log in with your UMass Lowell credentials. Once logged in, under \u201cAccount Settings,\u201d click the \u201cRequest PI Account\u201d button. The Unity team will review your request and approve your PI account.</p>"},{"location":"transition.html#student-postdoc-accounts","title":"Student &amp; Postdoc Accounts","text":"<p>Students request access to PI groups by logging into the Unity Portal, navigating to \u201cMy PIs,\u201d and clicking the \u201cplus\u201d button to search for your group. Students and postdocs can\u2019t access Unity without an associated PI, so please ensure the PI account is created and approved before requesting an account.</p>"},{"location":"transition.html#timeline-overview","title":"Timeline Overview","text":""},{"location":"transition.html#migrating-data-to-unity-deadline-may-15-2023","title":"Migrating Data to Unity (deadline: May 15, 2023)","text":"<p>All data must be migrated to Unity by May 15, 2023. PIs will temporarily receive sufficient storage in the /project directory to move as much data from the shared cluster as desired. However, storage quotas will be enforced starting January 2024. Users may purchase additional storage at that time.</p>"},{"location":"transition.html#shared-cluster-shutdown-read-only-may-1-2023-final-shutdown-june-1-2023","title":"Shared Cluster Shutdown (read-only: May 1, 2023, final shutdown: June 1, 2023)","text":"<p>The Shared Cluster will be turned off permanently on June 1, 2023. However, the cluster will become read-only ahead of the shutdown on May 1, 2023. While you will be able to continue transferring data, you will no longer be able to run jobs or create new files.</p>"},{"location":"transition.html#onboarding-live-sessions","title":"Onboarding Live Sessions","text":"<p>To ensure a smooth transition from the Shared Cluster to Unity, we held two online workshops on accessing Unity, data transfer and storage, and running jobs. Links to the recordings and slides follow each workshop description.</p>"},{"location":"transition.html#unity-onboarding-workshop-part-1-access-and-data-storage-march-31-2023","title":"Unity Onboarding Workshop Part 1: Access and Data Storage (March 31, 2023)","text":"<p>Friday, March 31, 2023, 2:00 pm to 3:00 pm (time for questions following) We provided an overview of Unity in comparison to the Shared cluster, explained the account request process for PIs/faculty and students/postdocs, and discussed storage on Unity and data transfer options.</p> <p>Recording link Slides link</p>"},{"location":"transition.html#unity-onboarding-workshop-part-2-running-jobs-april-14-2023","title":"Unity Onboarding Workshop Part 2: Running Jobs (April 14, 2023)","text":"<p>Friday, April 14, 2023, 2:00 pm to 3:00 pm (time for questions following) We discussed the differences between the Shared Cluster\u2019s LSF scheduler and Unity\u2019s Slurm scheduler, introduced Unity\u2019s partition layout, showed example Slurm jobs, and demonstrated Unity\u2019s Open OnDemand portal.</p> <p>Recording link Slides link</p>"},{"location":"transition.html#technical-information","title":"Technical Information","text":""},{"location":"transition.html#data-transfer","title":"Data Transfer","text":"<p>For information about moving data to Unity, see our file transfer documentation. Please give yourself ample time to move your data ahead of the May 1, 2023 read-only freeze.</p>"},{"location":"transition.html#job-scheduler","title":"Job Scheduler","text":"<p>While the Shared Cluster uses LSF to allocate resources, Unity uses the Slurm scheduler. See this quick reference page for a comparison between LSF and Slurm options.</p> <p>A full list of Slurm commands and their equivalents can be found in this PDF.</p>"},{"location":"MGHPCC/MGHPCC.html","title":"MGHPCC Transitions","text":"<p>Latest updates on MGHPCC transitions available here.</p> <p>*** For Users Moving from Shared/MGHPCC to Unity ***</p> <p>The Unity cluster uses Slurm, an open-source batch system.</p> <p>Slurm offers the same basic functionalities as LSF: you can use it to submit jobs, monitor their progress, and kill them if necessary. Similar to LSF, a job can be a single command, a parallel program using MPI or openMP, or a complex script. Slurm also supports GPUs and advanced features like job arrays.  Slurm works in the same way as LSF in that: you only need to specify the resources needed by your job, such as number of cores and GPUs (if applicable), memory, run-time, etc. Slurm will analyze your job's requirements and will automatically send it to the right partition. (Slurm uses partitions instead of queues, but the idea is the same.)</p> <p>The LSF commands that you use today to submit and monitor jobs \u2014 <code>bsub, bjobs</code> \u2014 and their various options will need to be replaced with their Slurm equivalent, which can be found here.</p> <p>A full list of Slurm commands and their equivalents can be found here.</p> <p>*** How to Transfer Files From One HPC to Another ***</p> <p>File transfer solutions can be found in our documentation here.</p>"},{"location":"Software%20%26%20Tools/ColabFold-notebook.html","title":"ColabFold notebook","text":"<pre><code># enter protein sequence below\nquery_sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'\n</code></pre> <pre><code># choose a template\n#`none` = no template information is used. \n#`pdb70` = detect templates in pdb70. \n#`custom` - upload and search own templates (PDB or mmCIF format, see [notes below](#custom_templates))\ntemplate_mode = \"none\" #@param [\"none\", \"pdb70\",\"custom\"]\n</code></pre> <pre><code>display_images = True #@param {type:\"boolean\"}\nimport os\nimport re\nimport hashlib\nimport random\nfrom sys import version_info \npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>if template_mode == \"pdb70\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n</code></pre> <pre><code># remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n</code></pre> <pre><code># create output directory\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\njobname = 'test' #@param {type:\"string\"}\n# remove whitespaces\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\nos.makedirs(jobname, exist_ok=True)\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\n# create csv file to store sequence\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n</code></pre> <pre><code># choose number of models to use\nnum_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\nuse_amber = num_relax &gt; 0\n# template mode --&gt; `none` = no template information is used. `pdb70` = detect templates in pdb70. `custom` - upload and search own templates (PDB or mmCIF format, see [notes below](#custom_templates))\ntemplate_mode = \"none\" #@param [\"none\", \"pdb70\",\"custom\"]\nmsa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\npair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\nmodel_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\"]\nnum_recycles = \"auto\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\nrecycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\nmax_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\nnum_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\nuse_dropout = False #@param {type:\"boolean\"}\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\nsave_all = False #@param {type:\"boolean\"}\nsave_recycles = False #@param {type:\"boolean\"}\n#set dpi for image resolution\ndpi = 200 #@param {type:\"integer\"}\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n</code></pre> <pre><code># decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n  print(a3m_file)\n\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n</code></pre> <pre><code>def input_features_callback(input_features):  \n  if display_images:    \n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nif 'logging_setup' not in globals():\n    setup_logging(Path(os.path.join(jobname,\"log.txt\")))\n    logging_setup = True\n\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\nprint(model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,    \n    model_type=model_type,\n    num_models=5,\n    num_recycles=num_recycles,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2,3,4,5],\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n</code></pre> <pre><code>#@title Display 3D structure {run: \"auto\"}\nimport py3Dmol\nimport glob\nimport matplotlib.pyplot as plt\nfrom colabfold.colabfold import plot_plddt_legend\nfrom colabfold.colabfold import pymol_color_list, alphabet_list\nrank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\ncolor = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\nshow_sidechains = False #@param {type:\"boolean\"}\nshow_mainchains = False #@param {type:\"boolean\"}\n\ntag = results[\"rank\"][0][rank_num - 1]\njobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\npdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\npdb_file = glob.glob(pdb_filename)\n\ndef show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n  model_name = f\"rank_{rank_num}\"\n  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n\n  if color == \"lDDT\":\n    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n  elif color == \"rainbow\":\n    view.setStyle({'cartoon': {'color':'spectrum'}})\n  elif color == \"chain\":\n    chains = len(queries[0][1]) + 1 if is_complex else 1\n    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n\n  if show_sidechains:\n    BB = ['C','O','N']\n    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n  if show_mainchains:\n    BB = ['C','O','N','CA']\n    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n\n  view.zoomTo()\n  return view\n\nshow_pdb(rank_num, show_sidechains, show_mainchains, color).show()\nif color == \"lDDT\":\n  plot_plddt_legend().show() \n</code></pre> <pre><code>#@title Plots {run: \"auto\"}\nfrom IPython.display import display, HTML\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\npae = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\"))\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\nplddt = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(HTML(f\"\"\"\n&lt;style&gt;\n  img {{\n    float:left;\n  }}\n  .full {{\n    max-width:100%;\n  }}\n  .half {{\n    max-width:50%;\n  }}\n  @media (max-width:640px) {{\n    .half {{\n      max-width:100%;\n    }}\n  }}\n&lt;/style&gt;\n&lt;div style=\"max-width:90%; padding:2em;\"&gt;\n  &lt;h1&gt;Plots for {escape(jobname)}&lt;/h1&gt;\n  &lt;img src=\"{pae}\" class=\"full\" /&gt;\n  &lt;img src=\"{cov}\" class=\"half\" /&gt;\n  &lt;img src=\"{plddt}\" class=\"half\" /&gt;\n&lt;/div&gt;\n\"\"\"))\n</code></pre> <pre><code># close session\n%system scancel $SLURM_JOB_ID\n</code></pre>"},{"location":"Software%20%26%20Tools/ColabFold.html","title":"ColabFold","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#colabfold-1-is-a-software-developed-to-accelerate-the-prediction-of-protein-3d-structures-and-protein-complexes-by-integrating-the-fast-search-algorithm-mmseqs2-with-alphafold2-2-or-rosettafold","title":"ColabFold [1] is a software developed to accelerate the prediction of protein 3D structures and protein complexes by integrating the fast search algorithm MMSeqs2 with AlphaFold2 [2] or RoseTTAFold.","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#colabfold-is-available-on-unity-by-using-a-jupyter-notebook-or-a-batch-script-both-methods-make-use-of-one-graphics-processing-unit-gpu-and-the-alphafold2-ai-tool-the-output-includes-the-predicted-protein-structure-in-a-pdb-format-text-file-along-with-files-to-evaluate-the-results","title":"ColabFold is available on Unity by using a Jupyter notebook or a batch script. Both methods make use of one graphics processing unit (GPU) and the AlphaFold2 AI tool. The output includes the predicted protein structure in a PDB format text file along with files to evaluate the results.","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#the-notebook-is-designed-to-run-colabfold-with-one-protein-sequence-while-the-batch-script-can-be-used-to-make-predictions-for-multiple-protein-sequences-at-once","title":"The notebook is designed to run ColabFold with one protein sequence while the batch script can be used to make predictions for multiple protein sequences at once.","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#colabfold-on-unity-is-currently-available-in-a-beta-version-for-questions-please-send-an-email-to-hpcumassedu","title":"ColabFold on Unity is currently available in a beta version. For questions, please send an email to hpc@umass.edu.","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#using-a-jupyter-notebook-to-access-colabfold","title":"Using a Jupyter notebook to access ColabFold","text":"<p>Start by accessing JupyterLab using the Unity OnDemand interface:</p> <p>https://ood.unity.rc.umass.edu/pun/sys/dashboard/batch_connect/sys/jupyterlab/session_contexts/new</p> <p>Click on the JupyterLab interactive app and fill out the following fields:</p> <ol> <li>The <code>Partition</code> field indicates the type of compute nodes to run your interactive session on. One of the gpu partitions should be selected to run ColabFold Jupyter notebook (gpu, gpu-long, uri-gpu or gpu-preempt).</li> <li>The <code>Maximum job duration</code> field defines how long the interactive session with JupyterLab should run for. This field can be left with the default value of one hour (1:00:00) for short protein sequences but should be increased to make predictions on larger protein sequences.</li> <li>The <code>Memory (in GB)</code> field defines the amount of memory in gigabytes allocated to your interactive session. To give you an idea of how much memory you may need, 8GB is enough for a protein of 59 amino acids but 50 GB is required for a large protein of 2894 amino acids.</li> <li>The <code>GPU count</code> field is the number of GPUs allocated to your interactive session. It should be set to 1 since ColabFold only runs on a single GPU.</li> <li>The <code>Modules</code> field corresponds to a list of modules to load. The two following modules should be added (separated only by a space) to this field in order to use the GPU: cudnn/cuda11-8.4.1.50 cuda/11.4.0</li> </ol> <p>The fields <code>CPU thread count</code> and <code>Extra arguments for Slurm</code> can be left blank.</p> <p>Inside JupyterLab: 1. Open the ColabFold.ipynb notebook. 2. Choose Python (colabfold) for the kernel. 3. Insert your protein sequence next to query_sequence and execute the code in the cell (press SHIFT+ENTER or press the play button in the toolbar above). 4. Run the code in the remaining cells in order to predict the protein structure with the default parameters (see Notes section below) and output plots and a visualization of the 3D structure. 5. The output directory containing the results will be located in the folder where you put the ColabFold.ipynb notebook.</p>"},{"location":"Software%20%26%20Tools/ColabFold.html#notes","title":"Notes:","text":"<ul> <li>ColabFold's notebook is setup to run with the following parameters that can be adjusted by the user:<ul> <li>No templates</li> <li>Number of models: 5</li> <li>Stop predictions at score 100</li> <li>Msa mode: mmseqs2_uniref_env</li> <li>Model type: alphafold2_ptm</li> </ul> </li> <li>The Jupyter notebook made available here is a modified version of the AlphaFold2_mmseqs2 notebook [3].</li> </ul>"},{"location":"Software%20%26%20Tools/ColabFold.html#using-a-batch-script-to-run-colabfold","title":"Using a batch script to run ColabFold","text":"<p>The example of batch scripts provided below should be considered for users dealing with a large number of protein sequences.</p>"},{"location":"Software%20%26%20Tools/ColabFold.html#colabfold-on-unity-has-3-main-components","title":"ColabFold on Unity has 3 main components:","text":"<ol> <li>A script colabfold_search that searches the ColabFold databases of proteins using MMseqs2 to build diverse multiple sequence alignments in A3M format.</li> <li>A script colabfold_batch designed to predict protein structures.</li> <li>A set of protein databases:<ul> <li>UniRef30: database containing 30% sequence identity clustered proteins based on UniRef100 non-redundant protein sequence database. </li> <li>Environmental database (also called ColabFoldDB): combination of the Big Fantastic Database (BFD) and the MGnify database with redundancy reduced in addition to metagenomic protein catalogs containing eukaryotic proteins, phage catalogs and an updated version of MetaClust.</li> <li>Templates database (PDB70): database containing 70% sequence identity clustered proteins from the Protein Data Bank (PDB) database.</li> </ul> </li> </ol>"},{"location":"Software%20%26%20Tools/ColabFold.html#getting-started","title":"Getting started","text":""},{"location":"Software%20%26%20Tools/ColabFold.html#search-against-the-colabfold-databases","title":"Search against the ColabFold databases","text":"<p>Finding homologous proteins using MMSeqs2 can be done by running a batch script using the SBATCH command: <code>sbatch &lt;path to batch script file&gt;</code></p> <p>The code below is an example of a batch script to run MMSeqs2. The top of the script contains the instruction to use bash to execute the commands (<code>#!/bin/bash</code>) and the SBATCH parameters (<code>#SBATCH &lt;parameter&gt;</code>) followed by the different modules required to run MMSeqs2 through ColabFold.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=cpu\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem=200G\n#SBATCH -t 05:00:00\n#SBATCH -o slurm-%j.out\n#SBATCH -e slurm-%j.err\n\nmodule load uri\nmodule load MMseqs2/14-7e284-gompi-2021b\nmodule load miniconda\nsource ~/.bashrc\nconda activate colabfold\n</code></pre> <p>The command colabfold_search shown below is added after activating the conda environment <code>colabfold-v1.5.0</code>. In this case, protein sequences contained in a fasta file are aligned against the UniRef30 (<code>--db1 uniref30_2202/uniref30_2202_db</code>) and environmental (<code>--db3 colabfold_envdb_202108/colabfold_envdb_202108_db</code>) databases. UniRef30 is the default database used to search proteins. In order to use the environmental database, the parameter <code>--use-env</code>has to be set to 1 in addition to providing the path (<code>--db3 colabfold_envdb_202108/colabfold_envdb_202108_db</code>).</p> <pre><code>colabfold_search &lt;path to fasta file&gt; /datasets/bio/colabfold &lt;path to output directory&gt; --db1 uniref30_2202/uniref30_2202_db --db3 colabfold_envdb_202108/colabfold_envdb_202108_db --use-env 1 --use-templates 0\n</code></pre> <p>To use the PDB70 templates database, the parameter <code>--use-templates</code> should be set to 1 and the path to the database should be provided with <code>--db2 pdb</code>.</p>"},{"location":"Software%20%26%20Tools/ColabFold.html#notes_1","title":"Notes:","text":"<ul> <li><code>&lt;path to fasta file&gt;</code> is the full path to a fasta file containing protein sequence(s) of interest.</li> <li><code>&lt;path to output directory&gt;</code> is the full path to an existing directory used to store the multiple sequence alignments (MSAs).</li> <li> <p>Note that it is recommended to request at least 200G using <code>#SBATCH --mem=200G</code> in order to load the protein databases.</p> </li> <li>Running colabfold_search with 1,762 proteins, the UniRef30 and environmental databases and the highest mmseqs sensitivity (s = 8) on a gpu A100 node with 64 threads takes approximately 3h.</li> </ul>"},{"location":"Software%20%26%20Tools/ColabFold.html#train-models-and-make-predictions-with-colabfold-using-a-batch-script","title":"Train models and make predictions with ColabFold using a batch script","text":"<p>A batch script can be used to train models and make predictions with ColabFold. It should be noted that predictions on proteins longer than 1000bp should be run on a GPU node with at least 40GB VRAM and that the whole process can be expedited on a large set of input protein sequences by submitting the batch script as an array job.</p> <p>The code below provides an example on how to make predictions using <code>colabfold_batch</code> in a batch script:</p> <p>The parameter --stop-at-score is used to stop generating models until the predicted confidence metric (pLDDT or predicted local distance difference test) is reached.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=uri-gpu\n#SBATCH --nodes=1\n#SBATCH --cpus-per-gpu=16\n#SBATCH -mem-per-gpu=80\n#SBATCH -t 05:00:00\n#SBATCH -o slurm-%j.out\n#SBATCH -e slurm-%j.err\n\nmodule load cudnn/cuda11-8.4.1.50\nmodule load cuda/11.4.0\nmodule load miniconda\nsource ~/.bashrc\n\nconda activate colabfold\n\ncolabfold_batch &lt;path to directory containing MSAs&gt; &lt;path to output directory&gt; --stop-at-score 85 --msa-mode 'MMseqs2 (UniRef+Environmental)'\n</code></pre> <p>The <code>colabfold_batch</code> command above will create the following files in the provided output directory for each input protein sequence:</p> <ul> <li> <p><code>{*}_PAE.png</code> \u2192 2D plot of the Predicted Aligned Error (PAE) for each of the 5 trained models.</p> </li> <li> <p><code>{*}_coverage.png</code> \u2192 plot of the coverage of protein sequences to the query protein.</p> </li> <li> <p><code>{*}_plddt.png</code> \u2192 plot of the pLDDT (predicted local distance difference test) scores for each residue and the 5 trained models.</p> </li> <li> <p><code>{*}_predicted_aligned_error_v1.json</code> \u2192 raw data with PAE for all residue pairs for each of the 5 trained models.</p> </li> </ul> <p>The next 2 files are generated for the 5 trained models:</p> <ul> <li><code>{*}_unrelaxed_rank_1_model_1.pdb</code> \u2192 PDB format text file containing the predicted structure obtained from model 1.</li> <li><code>{*}_unrelaxed_rank_1_model_1_scores.json</code> \u2192 raw data with the pLDDT scores for each residue of the protein structure obtained from model 1.</li> </ul>"},{"location":"Software%20%26%20Tools/ColabFold.html#notes_2","title":"Notes:","text":"<ul> <li><code>&lt;path to directory containing MSAs&gt;</code> is the same as <code>&lt;path to the output directory&gt;</code> used with the <code>colabfold_search</code> command.</li> <li><code>&lt;path to output directory&gt;</code> is the full path to an existing directory used to store the results.</li> </ul>"},{"location":"Software%20%26%20Tools/ColabFold.html#full-list-of-parameters-for-colabfold_search-and-colabfold_batch","title":"Full list of parameters for colabfold_search and colabfold_batch","text":"<pre><code>colabfold_search [-h] [-s S] [--db1 DB1] [--db2 DB2] [--db3 DB3]\n                        [--use-env {0,1}] [--use-templates {0,1}]\n                        [--filter {0,1}] [--mmseqs MMSEQS]\n                        [--expand-eval EXPAND_EVAL] [--align-eval ALIGN_EVAL]\n                        [--diff DIFF] [--qsc QSC] [--max-accept MAX_ACCEPT]\n                        [--db-load-mode DB_LOAD_MODE] [--threads THREADS]\n                        query dbbase base\n\n  query                 fasta files with the queries.\n  dbbase                The path to the database and indices you downloaded\n                        and created with setup_databases.sh\n  base                  Directory for the results (and intermediate files)\n  -s S                  mmseqs sensitivity (1-8). Lowering this will result in a\n                        much faster search but possibly sparser msas \u2192 default = 8\n  --db1 DB1             path to a UniRef database on Unity\n  --db2 DB2             path to the Templates database on Unity\n  --db3 DB3             path to the Environmental database on Unity\n  --use-env {0,1}\n  --use-templates {0,1}\n  --filter {0,1}\n  --mmseqs MMSEQS       Location of the mmseqs binary\n  --expand-eval EXPAND_EVAL\n  --align-eval ALIGN_EVAL\n  --diff DIFF\n  --qsc QSC\n  --max-accept MAX_ACCEPT\n  --db-load-mode DB_LOAD_MODE \u2192 default = 0 (batch searches)\n  --threads THREADS\n</code></pre> <pre><code>colabfold_batch [-h]   [--stop-at-score STOP_AT_SCORE]\n                       [--stop-at-score-below STOP_AT_SCORE_BELOW]\n                       [--num-recycle NUM_RECYCLE]\n                       [--num-ensemble NUM_ENSEMBLE]\n                       [--random-seed RANDOM_SEED] [--num-models {1,2,3,4,5}]\n                       [--recompile-padding RECOMPILE_PADDING]\n                       [--model-order MODEL_ORDER] [--host-url HOST_URL]\n                       [--data DATA]\n                       [--msa-mode {MMseqs2 UniRef+Environmental),MMseqs2 (UniRef only,single_sequence}]\n                       [--model-type {auto,AlphaFold2-ptm,AlphaFold2-multimer-v1,AlphaFold2-multimer-v2}]\n                       [--amber] [--templates]\n                       [--custom-template-path CUSTOM_TEMPLATE_PATH] [--env]\n                       [--cpu] [--rank {auto,plddt,ptmscore,multimer}]\n                       [--pair-mode {unpaired,paired,unpaired+paired}]\n                       [--recompile-all-models]\n                       [--sort-queries-by {none,length,random}]\n                       [--save-single-representations]\n                       [--save-pair-representations] [--training]\n                       [--max-msa           {512:5120,512:1024,256:512,128:256,64:128,32:64,16:32}]\n                       [--zip] [--use-gpu-relax]\n                       [--overwrite-existing-results]\n                       input results\n\n--stop-at-score STOP_AT_SCORE\n                        Compute models until plddt (single chain) or ptmscore\n                        (complex) &gt; threshold is reached. This can make\n                        colabfold much faster by only running the first model\n                        for easy queries.\n--stop-at-score-below STOP_AT_SCORE_BELOW \u2192 default = 0\n                        Stop to compute structures if plddt (single chain) or\n                        ptmscore (complex) &lt; threshold. This can make\n                        colabfold much faster by skipping sequences that do\n                        not generate good scores.\n--num-recycle NUM_RECYCLE &lt;strong&gt;\u2192 default = 3&lt;/strong&gt;\n                        Number of prediction cycles.Increasing recycles can\n                        improve the quality but slows down the prediction.\n--num-ensemble NUM_ENSEMBLE &lt;strong&gt;\u2192 default = 1&lt;/strong&gt;\n                        Number of ensembles.The trunk of the network is run\n                        multiple times with different random choices for the\n                        MSA cluster centers.\n--random-seed RANDOM_SEED &lt;strong&gt;\u2192 default = 0&lt;/strong&gt;\n                        Changing the seed for the random number generator can\n                        result in different structure predictions.\n--num-models {1,2,3,4,5} &lt;strong&gt;\u2192 default = 5&lt;/strong&gt;\n--recompile-padding RECOMPILE_PADDING &lt;strong&gt;\u2192 default = 1.1&lt;/strong&gt;\n                        Whenever the input length changes, the model needs to\n                        be recompiled, which is slow. We pad sequences by this\n                        factor, so we can e.g. compute sequence from length\n                        100 to 110 without recompiling. The prediction will\n                        become marginally slower for the longer input, but\n                        overall performance increases due to not recompiling.\n                        Set to 1 to disable.\n--model-order MODEL_ORDER\n--host-url HOST_URL &lt;strong&gt;\u2192 default = https://api.colabfold.com&lt;/strong&gt;\n--data DATA\n--msa-mode {MMseqs2 (UniRef+Environmental),MMseqs2 (UniRef only),single_sequence}\n                        Using an a3m file as input overwrites this option\n--model-type {auto,AlphaFold2-ptm,AlphaFold2-multimer-v1,AlphaFold2-multimer-v2}\n                        predict structure/complex using the following\n                        model.Auto will pick \"AlphaFold2\" (ptm) for structure\n                        predictions and \"AlphaFold2-multimer-v2\" for\n                        Complexes. &lt;strong&gt;\u2192 default = AlphaFold2-ptm&lt;/strong&gt;\n--amber               Use amber for structure refinement\n--templates           Use templates from pdb\n--custom-template-path CUSTOM_TEMPLATE_PATH\n                        Directory with pdb files to be used as input&lt;/code&gt;\n--env\n--cpu                 Allow running on the cpu, which is very slow\n--rank {auto,plddt,ptmscore,multimer} \u2192 default = plddt\n                        rank models by auto, plddt or ptmscore\n--pair-mode {unpaired,paired,unpaired+paired} \u2192 default =  unpaired+paired\n                        rank models by auto, unpaired, paired, unpaired+paired\n--recompile-all-models \u2192 default = false\n                        recompile all models instead of just model 1 and 3\n--sort-queries-by {none,length,random}\n                        sort queries by: none, length, random\n--save-single-representations\n                        saves the single representation embeddings of all\n                        models\n--save-pair-representations\n                        saves the pair representation embeddings of all models\n--training            turn on training mode of the model to activate drop\n                        Outs \u2192 default = false\n--max-msa {512:5120,512:1024,256:512,128:256,64:128,32:64,16:32} \u2192 default = null\n                        defines: `max_msa_clusters:max_extra_msa` number of\n                        sequences to use\n--zip                 zip all results into one &lt;jobname&gt;.result.zip and\n                        delete the original files\n--use-gpu-relax       run amber on GPU instead of CPU\n--overwrite-existing-results \u2192 default = false\n</code></pre>"},{"location":"Software%20%26%20Tools/ColabFold.html#references","title":"References","text":"<ol> <li>Mirdita, M., Sch\u00fctze, K., Moriwaki, Y. et al. ColabFold: making protein folding accessible to all. Nat Methods 19, 679\u2013682 (2022). https://doi.org/10.1038/s41592-022-01488-1</li> <li>Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583\u2013589 (2021). https://doi.org/10.1038/s41586-021-03819-2</li> <li>https://github.com/sokrypton/ColabFold#running-locally</li> </ol>"},{"location":"buy-in/requirements.html","title":"Hardware Requirements","text":"<p>Note</p> <p>This page is here for reference. Please do not purchase hardware intended for the Unity cluster until checking with the Unity team.</p>"},{"location":"buy-in/requirements.html#general-node-requirements","title":"General Node Requirements","text":"<p>Each server purchased is required to have the following hard requirements:</p> <ul> <li>IPMI 2.0 supported BMC with remote KVM built-in with discrete RJ45 management port</li> <li>Physical Ports<ul> <li>VGA Port</li> <li>USB Port<ul> <li>10G SFP+ Port or 25G SFP28 port<ul> <li>Note: If you require higher bandwidth for your nodes, that can be discussed, but will most likely require purchasing an addition switch (see below)</li> </ul> </li> <li>At least 250GB boot SSD (Solid-state drive)</li> <li>2x redundant PSU (Power supply unit)</li> </ul> </li> </ul> </li> </ul> <p>Each server purchased is recommended to have the following soft requirements:</p> <ul> <li>2x boot drive for redundancy</li> </ul>"},{"location":"buy-in/requirements.html#storage-node-requirements","title":"Storage Node Requirements","text":"<ul> <li>The storage controller should be an HBA card or a RAID card that can operate in HBA mode, since we will be using ZFS software raid.</li> <li>At least 2 data plane (10/25G) ports for redundant connection.</li> <li>2x boot drives for redundancy</li> </ul>"},{"location":"buy-in/requirements.html#network-equipment","title":"Network Equipment","text":"<p>On a case-by-case basis, it may be necessary to purchase network switches if enough hardware was purchased to span a rack.</p> <p>If that is the case, each rack requires the following for the top-of-rack data switch:</p> <ul> <li>For 10/25G Nodes Mellanox SN2410</li> <li>For 100G Nodes Mellanox SN2700</li> </ul> <p>In addition, a 1G switch is required for IPMI connection.</p> <ul> <li>For IPMI Any 1G switch that has L2 capability (Mellanox preferred)</li> </ul>"},{"location":"buy-in/types.html","title":"Types of Priority","text":"<p>Nodes you purchase will be included in the Unity ecosystem - meaning you'll be able to use the web portal, access all the same storage, and access JupyterLab. When purchasing hardware for integration with Unity, there are three types of \"priority\" that you can request for your hardware.</p> <p>Note</p> <p>The below only applies to compute hardware, storage hardware will almost always be owned by the purchasing PI only and not shared with users.</p>"},{"location":"buy-in/types.html#preemption-strictest","title":"Preemption (Strictest)","text":"<p>Priority nodes are added to: preempt partitions (cpu-preempt/gpu-preempt), and a newly created lab partition for use only by your lab</p> <p>Preemption is enabled in this mode. This means that if a priority user (member of the lab) wants to start a job but the nodes are full, non-priority jobs will be requeued on the spot for priority jobs. The requeued job will start again once resources are available. Your nodes are added to cpu/gpu-preempt partition depending on the type, in addition to the priority partition. This results in immediate access to the full capabilities of your own hardware, but the strictest limitations for general users of the cluster.</p>"},{"location":"buy-in/types.html#queueing-priority","title":"Queueing Priority","text":"<p>Priority nodes are added to: general partitions (cpu/gpu), and a newly created lab partition for use only by your lab</p> <p>Preemption is disabled in this mode, but your lab still gets queueing priority, which means if jobs are waiting to start on your hardware, your lab's jobs will always start first and take precedent. However, if your nodes are already full of general jobs, you will not be able to access this space until those jobs are done. As a result, we generally restrict general jobs to short queue timing (24 hours) such that priority users will not need to wait over a week etc. for non-priority jobs.</p>"},{"location":"buy-in/types.html#no-priority","title":"No Priority","text":"<p>In this mode no priority partition is created, and your nodes are contributed directly to the general partitions. It is not clear how this will affect billing for general hardware yet.</p>"},{"location":"connecting/jupyter.html","title":"JupyterHub Portal","text":"<p>The simplest method of using the cluster is using the Jupyter portal.</p> <p>You can access the jupyter portal from the Home Page &gt; \"JupyterLab\". Simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here.</p>"},{"location":"connecting/ood.html","title":"Connect via OpenOnDemand","text":"<p>The easiest way to connect to Unity is at ood.unity.rc.umass.edu.</p> <p>Unity OnDemand uses your organization's identity provider to give access to Unity. This means that you no longer need to install SSH, configure SSH, setup public and private keys, all you need to do is login with your browser like any other site. The end result is the same.</p>"},{"location":"connecting/ood.html#getting-a-login-shell","title":"Getting a Login Shell","text":"<p>Login to Unity OnDemand with your organization's identity provider, and click on <code>Unity Shell Access</code>.</p> <p></p> <p>It should open a new tab that looks like this:</p> <p></p> <p>You can also customize shell themes:</p> <p></p>"},{"location":"connecting/ssh.html","title":"SSH Connection","text":"<p>The most traditional method of connecting to Unity is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will likely be using on Unity. SSH stands for \"secure shell\".</p>"},{"location":"connecting/ssh.html#configure-ssh-keys","title":"Configure SSH Keys","text":"<p>The authentication method we use for SSH connections is with public/private RSA keys. You can read more about the public/private key exchange here.</p> <p>For the purposes of this guide, you should know that there is a public key which is stored on the server, and a private key, which you keep on your local computer. Think of them like your name and your social security number, respectively. In very basic terms, you authenticate the public key with your private key and that allows you to login to Unity.</p> <p>You must save your public key on Unity by adding it in your account settings. If you are unsure how to generate a public/private key pair, simply click on 'Generate Key'. The public key will be added to our database, and the private key will be downloaded to your computer.</p> <p>Note</p> <p>It's recommended that you place this downloaded private key in your home directory's <code>.ssh</code> folder. This is <code>C:/Users/YOUR_NAME/.ssh</code> in Windows, <code>/home/YOUR_NAME/.ssh</code> in Linux, and <code>/Users/YOUR_NAME</code> in Mac. In the terminal, a shortcut for this directory is the <code>~</code> symbol.</p> <p>This command will make the move on any operating system:</p> <p><code>mv ~/Downloads/privkey.key ~/.ssh/unity-privkey.key</code></p> <p>On Linux/Mac, you will need to change the permissions on the file due to its importance to security.</p> <p><code>chmod 600 ~/.ssh/unity-privkey.key</code></p> <p>It's recommended that you also add a password to this file using the following command:</p> <p><code>ssh-keygen -p -f ~/.ssh/unity-privkey.rsa</code></p>"},{"location":"connecting/ssh.html#connection-details","title":"Connection Details","text":"<p>If you know what to do with this information already, you can skip the rest of this guide.</p> <ul> <li>Hostname/Address: <code>unity.rc.umass.edu</code></li> <li>Username: <code>NETID_school_edu</code></li> </ul> <p>Note</p> <p>Your username is in the format <code>&lt;organization username&gt;_&lt;organization&gt;_edu</code>. View your username here</p>"},{"location":"connecting/ssh.html#cli-users","title":"CLI Users","text":"<p>We recommend connecting to Unity via the terminal. Windows, Mac, and most distributions of linux come with the OpenSSH client, which you can use to connect to Unity in your terminal.</p> <p>If the file <code>~/.ssh/config</code> doesn't exist, create it. Copy the following contents to your Notepad and replace <code>&lt;NETID&gt;</code> and <code>&lt;PATH_TO_PRIVATE_KEY&gt;</code> to your specifications: Remember to save the file in a directory of your choosing, without an extension.</p> <pre><code>Host unity\n     HostName unity.rc.umass.edu\n     User &lt;USERID&gt;_&lt;ORGANIZATION&gt;_edu\n     IdentityFile &lt;PATH_TO_PRIVATE_KEY&gt;\n</code></pre> <p>Note</p> <p>Doing this with a text editor and a file explorer can be challenging because these user friendly methods don't like files without an extension, and the ssh config file must not have an extension.</p> <p>In Windows Notepad, you can save a file with no extension in the 'All Files' category, and windows will add the <code>.txt</code> extension regardless, which won't work.</p> <p>The Mac TextEdit doesn't even have the option to save as <code>.txt</code>, which is tremendously unhelpful. You can make your current file plain-text formatted using \u2318-\u21e7-T, and you can add plain-text as a 'Save as' option in the config.</p> <p>The most reliable way to put your OpenSSH config file in the correct location is to open the terminal and use the <code>mv</code> (move) command, which will rename files with no fuss.</p> <p><code>mv path/to/source-file path/to/desination-file</code></p> <p><code>mv ~/Desktop/ssh-config.txt ~/.ssh/config</code></p> <p>Once the OpenSSH config file is in place, you can connect to Unity in your terminal using the command <code>ssh unity</code>.</p> <p></p>"},{"location":"connecting/ssh.html#windows-gui-users","title":"Windows GUI Users","text":"<p>Windows users can use PuTTY to connect to Unity. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work.</p> <p>Open PuTTY and enter hostname <code>unity.rc.umass.edu</code> on the main page</p> <p></p> <p>On the left sidebar, navigate to connection -&gt; data, and enter your username.</p> <p></p> <p>On the left sidebar, navigate to connection -&gt; ssh -&gt; auth, and browse to your private key location.</p> <p></p> <p>Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter <code>unity</code> as the profile name and click <code>save</code>. You can then double click on <code>unity</code> under saved sessions to connect to Unity right away.</p> <p></p> <p>The server's ssh-ed25519 key fingerprint is:</p> <p><code>ssh-ed25519 255 SHA256:jC7BF7h5/RJo5Svx1v+lufdf+I/ogu5dQV2sUe+y8ek</code></p> <p></p> <p>If this key matches what is on your terminal, go ahead and click \"Accept\".</p>"},{"location":"managing-files/cli.html","title":"Console - SCP/Rsync","text":""},{"location":"managing-files/cli.html#cli","title":"CLI","text":"<p>It's best to try this after you have already successfully connected to Unity with OpenSSH.</p> <p>As these are CLI procedures, the first thing you need to do is open your terminal and navigate to the directory (folder) where the files you want to upload are located. Alternatively you can use absolute paths in your command and skip this step.</p> <pre><code># Windows\ncd C:/Users/YOUR_NAME/Desktop\n# Linux\ncd /home/$USER/Desktop\n# Mac\ncd /Users/YOUR_NAME/Desktop\n</code></pre> <p>Assuming, of course, that the files you want to upload are located in your desktop directory. And in the Windows case, assuming that the drive you want to copy from is the C drive.</p> <p>Note</p> <p>If your file name contains spaces, you will have to put it in quotes.</p>"},{"location":"managing-files/cli.html#scp","title":"SCP","text":"<p>OpenSSH comes with the <code>scp</code> command, which uses the same argument structure as <code>cp</code> (copy) but with the added benefit of referencing the OpenSSH config file (<code>~/.ssh/config</code>). This is how I can use <code>unity</code> as part of a command, because the OpenSSH config file contains the connection information for host <code>unity</code>.</p> <pre><code># single file\nscp FILE_NAME unity:~\n\n# entire directory\nscp -r DIRECTORY_NAME unity:~\n</code></pre> <p>This will copy the files in question to your Unity home directory. You could also upload to elsewhere on the Unity filesystem, wherever you have permissions.</p> <p>Note</p> <p><code>-r</code> in many commands is short for 'recursive'. It means to recursively open directories to ensure that all contained files are accounted for.</p> <p>Note</p> <p><code>~</code> in the terminal represents your home directory.</p> <p>This is <code>C:/Users/YOUR_NAME</code> in Windows, <code>/home/YOUR_NAME</code> in Linux, and <code>/Users/YOUR_NAME</code> in Mac.</p>"},{"location":"managing-files/cli.html#rsync","title":"RSYNC","text":"<p><code>rsync</code> can be installed on Linux and Mac. The syntax is the same as <code>scp</code>. It also references the OpenSSH config file.</p> <p>It's recommended to use the <code>-tlp</code> flags so that timestamps, relative links, and permissions are preserved, respectively.</p> <pre><code># single file\nrsync -tlp FILE_NAME unity:~\n\n# entire directory\nrsync -rtlp DIRECTORY_NAME unity:~\n</code></pre>"},{"location":"managing-files/filezilla.html","title":"FileZilla","text":""},{"location":"managing-files/filezilla.html#filezilla","title":"FileZilla","text":"<p>FileZilla can use either an <code>.rsa</code> or a <code>.ppk</code> private key, but the 'Browse' button will show only <code>.ppk</code> files. To use an <code>.rsa</code> key, type in the path to the key file by hand.</p> <p>This guide assumes that your key lives at <code>~/.ssh/KEYFILE</code>, but you can substitute this path.</p> <p>You can install FileZilla here</p> <ul> <li>FileZilla may ask you if you want to install McAfee, you probably don't. If you don't have antivirus already, you probably should.</li> <li>The FileZilla installer executable can be sometimes marked as a virus, it isn't.</li> </ul> <p>Select the Site Manager in FileZilla:</p> <p></p> <p>Create a New Site:</p> <p></p> <p>Fill in the Fields:</p> <ul> <li>Type a name for the site under My Sites on the left</li> <li>Protocol: SFTP</li> <li>Host: unity.rc.umass.edu</li> <li>User: your email but replace the <code>.</code> and <code>@</code> with <code>_</code></li> <li>Key File: /path/to/your/keyfile</li> </ul> <p></p> <p>This configuration is saved automatically. You can use the 'Connect' button in the bottom right to open an explorer on the Unity Filesystem,and you can drag and drop your files across the two panels.</p> <p>Properly connected, FileZilla should look like this:</p> <p></p>"},{"location":"managing-files/globus.html","title":"Globus","text":""},{"location":"managing-files/globus.html#globus","title":"Globus","text":"<p>Globus Connect allows for transfers between Globus collections.</p> <p>This can be useful, for example, when migrating from one HPC cluster to another.</p> <p>How can I transfer files to and from my local machine with Globus?</p>"},{"location":"managing-files/globus.html#using-globus-connect-in-your-browser","title":"Using Globus Connect in your browser","text":"<p>Know which two Globus collections you want to transfer between.</p> <p>One is presumably Unity. The other could be your local machine if you install Globus Connect Personal. (see above)</p> <p>Go to app.globus.org</p> <p>If prompted, select your university and login with their identity provider.</p> <p></p> <p>Go to the File Manager and select a collection.</p> <p>Either collection involved in your transfer will do. At the time of this writing, there is more than one collection named Unity. The following string can be pasted into the search box to select our Globus endpoint: <code>acda5457-9c06-4564-8375-260ba428f22a</code></p> <p></p> <p>Once a collection is selected, there should be two mirrored panels. If not, select the split panel layout in the top right:</p> <p></p> <p>Select the other collection involved in your transfer. This will take you back to the collection search page.</p> <p>Of the two split panels, each has a Collection, a Path, a number of selected files, and a Start button to copy the selected files to the other side. Below is one panel:</p> <p></p> <p>Configure your transfer and press 'Start'.</p>"},{"location":"managing-files/hpc-workspace.html","title":"HPC Workspace","text":"<p>Unity provides a tool called HPC Workspace to allow you to create and manage scratch space in a sustainable fashion.</p>"},{"location":"managing-files/hpc-workspace.html#creating-a-workspace","title":"Creating a workspace","text":"<p>Creating a workspace is done with the <code>ws_allocate</code> command. To view all options, run <code>ws_allocate -h</code>.</p>"},{"location":"managing-files/hpc-workspace.html#examples","title":"Examples","text":""},{"location":"managing-files/hpc-workspace.html#create-simple-workspace-for-single-user-with-defaults","title":"Create simple workspace for single user with defaults","text":"<pre><code>username@login2:~$ ws_allocate simple\nInfo: creating workspace.\n/scratch/workspace/username-simple\nremaining extensions  : 3\nremaining time in days: 1\n</code></pre> <p>Now we can see the directory that was created.</p> <pre><code>username@login2:~$ ls -ld /scratch/workspace/username-simple\ndrwx------ 2 username username 4096 Mar  2 17:48\n</code></pre>"},{"location":"managing-files/hpc-workspace.html#send-email-reminder-before-workspace-expiration","title":"Send email reminder before workspace expiration","text":"<p>The <code>-m</code> option allows you to specify an email address to be notified prior to workspace expiration. When using this option, you are also required to specify the number of days prior to expiration via the <code>-r</code> option.</p> <pre><code>username@login2:~$ ws_allocate -m username@umass.edu -r 1 email 2\nInfo: creating workspace.\n/scratch/workspace/username-email\nremaining extensions  : 3\nremaining time in days: 2\n</code></pre>"},{"location":"managing-files/hpc-workspace.html#list-workspaces","title":"List workspaces","text":"<p>The <code>ws_list</code> command can be used to list your workspaces.</p> <pre><code>username@login2:~$ ws_list \nid: username-email\n     workspace directory  : /scratch/workspace/username-email\n     remaining time       : 0 days 23 hours\n     creation time        : Fri Mar  3 16:56:51 2023\n     expiration date      : Sat Mar  4 16:56:50 2023\n     filesystem name      : workspace\n     available extensions : 3\nid: username-simple\n     workspace directory  : /scratch/workspace/username-simple\n     remaining time       : 0 days 23 hours\n     creation time        : Fri Mar  3 16:56:44 2023\n     expiration date      : Sat Mar  4 16:56:44 2023\n     filesystem name      : workspace\n     available extensions : 3\n</code></pre>"},{"location":"managing-files/hpc-workspace.html#release-workspace","title":"Release workspace","text":"<p>Once you are done using a workspace, you can release it. Releasing a workspace means that the ID can be reused and the directory is not accessible.</p> <pre><code>username@login2:~$ ws_release email\nusername@login2:~$ ws_release simple\nusername@login2:~$ ws_list\nusername@login2:~$\n</code></pre> <p>Tip</p> <p>Please release workspaces when they are no longer needed to help optimize scratch space for all Unity users.</p> <p>Important</p> <p>Releasing a workspace does not delete the data immediately. The deletion of the workspace can take place at any time once it's released.</p>"},{"location":"managing-files/hpc-workspace.html#extend-workspace","title":"Extend workspace","text":"<p>You are allowed a limited number of opportunities to extend a workspace. You can see the available number of extensions in the output of <code>ws_list</code>.</p> <p>Create a workspace and extend it for 30 days.</p> <pre><code>username@login2:~$ ws_allocate extend\nInfo: creating workspace.\n/scratch/workspace/username-extend\nremaining extensions  : 3\nremaining time in days: 1\nusername@login2:~$ ws_allocate -x extend 30\nInfo: extending workspace.\n/scratch/workspace/username-extend\nremaining extensions  : 2\nremaining time in days: 30\n</code></pre>"},{"location":"managing-files/hpc-workspace.html#creating-a-group-workspace","title":"Creating a Group workspace","text":"<p>The HPC Workspace utility allows you to create shared workspaces which can be useful for collaborating on a project. In order to create a shared workspace, all members must be members of a common group. For example, it may be that all members of a particular PI group want to collaborate on a project.</p> <p>Create a group workspace that can be shared amongst a particular PI group.</p> <pre><code>username@login2:~$ ws_allocate -G pi_pi-username shared\nInfo: creating workspace.\n/scratch/workspace/username-shared\nremaining extensions  : 3\nremaining time in days: 0\n</code></pre> <p>Now you can see that the directory was created with group sharing permissions.</p> <pre><code>username@login2:~$ ls -ld /scratch/workspace/username-shared \ndrwxrws--- 2 username pi_pi-username 4096 Mar  3 19:08\n</code></pre>"},{"location":"managing-files/intro.html","title":"Uploading Files to the Unity Filesystem","text":"<p>The only way to add files to the Unity filesystem is through an SSL encrypted connection. It can be done with Unity OnDemand, FileZilla, Globus, or in the command line.</p> <ul> <li>Unity OnDemand is the most feature complete and intuitive interface, and is recommended.</li> <li>FileZilla is good for those who have their SSH keys set up and are familiar with the software already.</li> <li>Globus is recommended for those who already have access to other Globus resources.</li> <li><code>scp</code> and <code>rsync</code> are recommended for those who are comfortable in the command line and want to work quickly.</li> </ul> <p>Note</p> <p>Uploading files using American residential internet is typically very slow. UMass Amherst has a fiber line going directly to MGHPCC to improve speeds.</p>"},{"location":"managing-files/intro.html#your-key-file","title":"Your Key File","text":"<p>The FileZilla and Console methods require public/private SSH keys to be set up.</p> <p>When you set up your Unity account, you chose between PuTTY (<code>.ppk</code>) and OpenSSH. (<code>.rsa</code>)</p> <p><code>scp</code> and <code>rsync</code> use OpenSSH, and FileZilla prefers <code>.ppk</code> but can work with <code>.rsa</code>.</p> <p>Depending on which software you use, you can generate one of each. You can also convert between these keys using a program like PuttyGEN.</p> <p>Configuring SSH Keys</p> <p>Account Settings</p>"},{"location":"managing-files/ood.html","title":"OnDemand","text":"<p>Unity OnDemand is the easiest way to work with the Unity filesystem.</p> <p>Unity OnDemand uses your organization's identity provider to give access to Unity. This means that you no longer need to install SSH/FileZilla, configure SSH, setup public and private keys, all you need to do is login with your browser like any other site.</p>"},{"location":"managing-files/ood.html#you-can","title":"You can ...","text":"<ul> <li>navigate the filesystem</li> <li>copy, move, create, and delete files and directories</li> <li>upload and download files from/to your local machine</li> <li>edit text files</li> <li>view images</li> <li>open a given directory in an interactive session on a compute node</li> </ul>"},{"location":"managing-files/ood.html#opening-the-file-explorer","title":"Opening the File Explorer","text":"<p>Login to Unity OnDemand with your organization's identity provider, and click <code>Files &gt; Home Directory</code>.</p> <p></p> <p>It should change your tab to look like this:</p> <p></p>"},{"location":"managing-files/ood.html#navigating","title":"Navigating","text":"<p>Click on directories to open them. You can click on different parts of your current working directory to go back. For example, with <code>/home/simonleary_umass_edu</code> as my current working directory, I can go back to <code>/home</code> by clicking on it:</p>"},{"location":"managing-files/ood.html#other-functions","title":"Other functions","text":"<p>You can edit a text file by clicking on it. You can view an image by clicking on it. You can open your current working directory in an interactive session with the <code>Open in Terminal</code> button. Most of the functions listed above are buttons to be pressed at the top of the file explorer. These should be mostly self explanatory.</p>"},{"location":"managing-files/ood.html#_1","title":"OnDemand","text":""},{"location":"slurm/index.html","title":"Introduction to Slurm: The Job Scheduler","text":"<p>Slurm is the job scheduler we use in Unity. More info about what a job scheduler is can be found in the introduction. Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user need to know should be available.</p> <p>Note</p> <p>Doing work on the login nodes can cause Unity to become sluggish for the entire user base. We have disincentivized this by setting limits on cpu and memory. Learn how to use <code>srun</code> interactive sessions to switch from a login node to a compute node.</p>"},{"location":"slurm/index.html#core-limits","title":"Core Limits","text":"<p>There is currently a 300 CPU core, 64 GPU limit to be shared by the users of each lab.</p> <p>When you try to go over this limit, you will be denied for <code>MaxCpuPerAccount</code>.</p>"},{"location":"slurm/index.html#partitions-queues","title":"Partitions / Queues","text":"<p>Our cluster has a number of slurm partitions defined, also known as a queue. As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. Find out which partition is best for your job here.</p>"},{"location":"slurm/index.html#jobs","title":"Jobs","text":"<p>A job is an operation which the user submits to the cluster to run under allocated resources. There are two commands for this, <code>srun</code> and <code>sbatch</code>. <code>srun</code> is tied to your current session, and can allow you to interact with your job. <code>sbatch</code> is not tied to your current session, so you can start it and walk away. If you want to interact with your job and be able to walk away, you can use <code>tmux</code> to make a detachable session. (see below)</p>"},{"location":"slurm/index.html#srun","title":"SRUN","text":"<p>An <code>srun</code> job is tied to your ssh session. If you break (ctrl+C) or close your ssh session during an <code>srun</code> job, the job will be killed.</p> <p>You can also make an interactive job, which will allow your job to take input from your keyboard. You can run <code>bash</code> in an interactive job to resume your work on a compute node just as you would on a login node. This is highly recommended.</p> <p>See SRUN Jobs for more information.</p>"},{"location":"slurm/index.html#sbatch","title":"SBATCH","text":"<p>An <code>sbatch</code> job is submitted to the cluster with no information returned to the user other than a Job ID. An <code>sbatch</code> job will try to create a file in your current working directory that contains the results of your job.</p> <p>See SBATCH Jobs for more information.</p>"},{"location":"slurm/index.html#tmux-srun","title":"TMUX SRUN","text":"<pre><code>tmux\n# tmux session opens\nsrun --pty -c 1 bash\n# interactive job on compute node opens with one cpu core\nsleep 3600; echo \"done\"\n# interactive job will have blinking cursor for an hour\n# &gt; ctrl+b\n# tmux keyboard-shortcut command mode opens\n# &gt; d\n# tmux session detaches, back to login node\n# at this point you can log off and log back in without killing the job\ntmux ls\n# print list of tmux sessions\n# first number on the left (call it X) is needed to re-attach the session\ntmux attach-session -t X\n# back to interactive job\n</code></pre>"},{"location":"slurm/arrays.html","title":"Array Batch jobs","text":"<p>It is often the case that you need to run the same set of commands over either different sets of data, or the same data with different parameters. Array jobs help you submit several jobs at once with the same batch script, and provide a relative index (via environment variable <code>$SLURM_ARRAY_TASK_ID</code>) that can be used by the script or program to select what to do .</p>"},{"location":"slurm/arrays.html#syntax","title":"Syntax","text":"<p>To submit an array job, specify</p> <pre><code>--array=&lt;indexes&gt;\n</code></pre> <p>where <code>&lt;indexes&gt;</code> can be any combination of ranges and lists. For example, <code>1-100,200</code> would submit 101 jobs. Ranges support skip counting with a <code>:</code>, so <code>0-15:4</code> is the same as <code>0,4,8,12</code>.</p> <p>When submitting a large array it may be useful to limit the number of jobs in the array running at one time (see the partition list for per-user and per-account limits). For this,you can use the <code>%</code> syntax. For example, this will limit to 10 concurrent jobs:</p> <pre><code>--array=1-100%10\n</code></pre> <p>This value can be updated later with <code>scontrol</code>:</p> <pre><code>scontrol update jobid=.... arraytaskthrottle=15\n</code></pre> <p>Note</p> <p>Decreasing the number of concurrent jobs will not kill job. A new job will not start until the running tasks dips below the new threshold.</p> <p>Jobs submitted as an array will be listed using the <code>&lt;jobid&gt;_&lt;arrayid&gt;</code> syntax in <code>squeue</code> and <code>sacct</code> output, and default output file name will be <code>slurm-%A_%a.out</code> to match this.</p>"},{"location":"slurm/arrays.html#selecting-data-or-parameters-with-slurm_array_task_id","title":"Selecting data or parameters with SLURM_ARRAY_TASK_ID","text":"<p>Each job in the job array will have a unique value from the set of indexes provided. Here are some examples of how to make use of that value.</p>"},{"location":"slurm/arrays.html#selecting-a-dataset","title":"Selecting a dataset","text":"<p>Depending on how your data is structured, choosing which set to work on mean selecting a directory or file name to work on. Best practice here would be to generate a list ahead of time, and then select an item from that list:</p> <pre><code>login$ ls -1 *.fasta &gt; input.txt\nlogin$ wc -l input.txt\n23\nlogin$ sbatch --array=1-23 search.sh\n</code></pre> <p>Then in the batch script, use <code>$SLURM_ARRAY_TASK_ID</code> to select a line</p> <pre><code>#!/bin/bash\n#$BATCH -N 1 -c 12\nINPUT_FILE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" input.txt)\nmodule load blast-plus/2.12.0\nblastx -query \"${INPUT_FILE}\" -db nr -num_threads \"${SLURM_CPUS_ON_NODE}\"\n</code></pre> <p>Note</p> <p>Do not use the <code>ls</code> command directly in the batch script, as changes to the contents of the directory during the run may affect the order of the output, leading to missed or double-processed files.</p>"},{"location":"slurm/arrays.html#selecting-a-parameter-set","title":"Selecting a parameter set","text":"<p>The environment variable can also be used directly in your code. For example:</p> <pre><code>import os\n# Define the set of parameters\nparameters = [\n    {'x': 1, 'y': 1},\n    {'x': 2, 'y': 1},\n]\ndef model(x, y):\n    pass\nif __name__ == \"__main__\":\n    # Note: use zero-based indexing on submit: --array=0-1\n    DATA_SET = int(os.getenv(\"SLURM_ARRAY_TASK_ID\"))\n    model(**parameters[DATA_SET])\n</code></pre>"},{"location":"slurm/arrays.html#using-as-random-number-generator-seed","title":"Using as random number generator seed","text":"<p>For some applications the only change in the code might be a seed for a random number generator. This should work the same as the previous example:</p> <pre><code>import os\nimport random\nSEED = int(os.getenv(\"SLURM_ARRAY_TASK_ID\"))\nrandom.seed(SEED)\n# Remember the value somewhere so you can reproduce\nprint(f\"Using RNG Seed: {SEED}\")\n</code></pre>"},{"location":"slurm/arrays.html#email-from-job-arrays","title":"Email from Job arrays","text":"<p>By default emails for jobs BEGIN/END/FAIL apply to the array as a whole. For small arrays you might consider <code>--mail-type=ARRAY_TASKS,FAIL</code> to be notified of each task failure, if any.</p>"},{"location":"slurm/arrays.html#canceling-an-array-job","title":"Canceling an array job","text":"<p>You can cancel either a specific array index, or an entire array (running and pending), depending on how you specify the job id:</p> <pre><code>scancel jobid_arrayid  # cancel a specific instance\nscancel jobid  # will cancel any running or pending items in the array\n</code></pre>"},{"location":"slurm/arrays.html#job-dependencies","title":"Job dependencies","text":"<p>If your array jobs also have multiple steps with different resource requirements, you can submit multiple job arrays where the next step depends only on the corresponding ID of the last array. To do this you need to submit each job sequentially, and use <code>--dependency=aftercorr:</code> to specify it on the next job. For example, if you have a job that requires a lot of resources to compute, but only a single core to post-process, you can use the following commands (either on the command line or in another bash script).</p> <pre><code>mainjob=$(sbatch -P --array=0-12%6 -G 1 -p gpu main_job.sh)\nsbatch --array=0-12%6 --dependency=aftercorr:$mainjob post_process.sh\n</code></pre>"},{"location":"slurm/mpi.html","title":"Running Jobs on Multple Nodes Using MPI","text":""},{"location":"slurm/mpi.html#this-is-the-simplified-sanity-check-that-the-unity-admins-used-to-verify-that-mpi-is-working","title":"This is the (simplified) sanity check that the Unity admins used to verify that MPI is working:","text":"<p>Note</p> <p>We recommend when running quick jobs that you put your job in a <code>preempt</code> partition for a shorter wait time.</p> <p><code>srun -p cpu-preempt ...</code></p>"},{"location":"slurm/mpi.html#srun","title":"srun","text":"<pre><code>module load openmpi\nmpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array\nsrun --pty -N 2 mpi_array\n</code></pre>"},{"location":"slurm/mpi.html#sbatch","title":"sbatch","text":"<pre><code>module load openmpi\nmpicc /modules/admin-resources/mpi_testing/mpi_array.c -o mpi_array\nsbatch -N 2 mpi_script\n</code></pre> <p>Where <code>mpi_script</code> is a file containing the following:</p> <pre><code>#!/bin/bash\nsrun mpi_array\n</code></pre>"},{"location":"slurm/mpi.html#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"slurm/mpi.html#why-cant-i-use-mpirunmpiexec","title":"Why can't I use <code>mpirun</code>/<code>mpiexec</code>?","text":"<pre><code>This version of Spack (openmpi ~legacylaunchers schedulers=slurm)\nis installed without the mpiexec/mpirun commands to prevent\nunintended performance issues. See https://github.com/spack/spack/pull/10340\nfor more details.\nIf you understand the potential consequences of a misconfigured mpirun, you can\nuse spack to install 'openmpi+legacylaunchers' to restore the executables.\nOtherwise, use srun to launch your MPI executables.\n</code></pre> <p>The community of HPC admins at Spack have agreed that using <code>mpirun</code> with slurm is a bad idea. <code>srun</code> is capable of doing all that <code>mpirun</code> is, and having the two fight over control is reported to cause poor performance. We currently have a module called <code>openmpi+mpirun</code>, which was installed in Spack with <code>+legacylaunchers</code>, enabling you to use the wrappers as you please.</p>"},{"location":"slurm/mpi.html#why-do-i-get-multiple-outputs-from-my-mpi-aware-binaries","title":"Why do I get multiple outputs from my mpi aware binaries?","text":"<p>This is because message passing is not working. You are running duplicates of your job in parallel, which are unaware of each other. If using <code>srun</code>, you should make sure that you have the pseudo-terminal enabled <code>--pty</code>. This should not occur with <code>sbatch</code>.</p>"},{"location":"slurm/mpi.html#why-do-i-keep-getting-pmix-error-no-permissions-in-file","title":"Why do I keep getting <code>PMIX ERROR: NO-PERMISSIONS in file</code>?","text":"<p>openmpi is dependent on pmix. Our current system slurm installation was not configured with pmix support. This is evident by <code>srun --mpi=list</code>. pmix_v2 and pmix_v3 should be there, but they aren't. We will likely recompile slurm to accommodate this.</p>"},{"location":"slurm/mpi.html#why-do-i-keep-getting-this-openfabrics-warning","title":"Why do I keep getting this OpenFabrics warning?","text":"<pre><code>\"No OpenFabrics connection schemes reported that they were able to be\nused on a specific port.  As such, the openib BTL (OpenFabrics\nsupport) will be disabled for this port.\"\n</code></pre> <p>We do not currently have Infiniband hardware in our network, and openmpi would like us to. You can simply add <code>-mca btl ^ofi</code> to your <code>mpirun</code> command and disable the Infiniband feature. We will likely recompile openmpi to disable this site-wide.</p> <pre><code>mpirun -mca btl ^ofi ...\n</code></pre>"},{"location":"slurm/ood.html","title":"OnDemand","text":"<p>Unity OnDemand gives you access to much of Slurm's functionality without any use of the command line.</p> <p>There are three main features of Unity OnDemand job management:</p> <ul> <li>Job Composer: Customize and schedule batch jobs</li> <li>Job Templater: Make templates from jobs and jobs from templates</li> <li>Active Job Viewer: List jobs, kill jobs, read output files</li> </ul> <p>Note</p> <p>One thing that OpenOnDemand lacks is a view of previously submitted jobs. The Active Job Viewer is based off of <code>squeue</code>, and a job only stays behind in <code>squeue</code> for 5 minutes after it is completed. (as of Jan. 2023)</p>"},{"location":"slurm/ood.html#job-composer","title":"Job Composer","text":"<p>You can open the job composer by clicking on <code>Jobs &gt; Job Composer</code> or by following this link.</p> <p></p> <p>It should open a new tab that looks like this:</p> <p></p>"},{"location":"slurm/ood.html#job-templates","title":"Job Templates","text":"<p>You can open the job templates by clicking on <code>Job Composer &gt; Templates</code> or by following this link.</p> <p></p> <p>It should change your tab to look like this:</p> <p></p> <p>Note</p> <p>You might notice that the job template descriptions are displayed \"literally\", with unwanted <code>&lt;p&gt;</code> and <code>&lt;code&gt;</code> tags. This is a bug in OpenOnDemand and should be patched soon.</p> <p>We have a number of job templates at your disposal. You can select a template and copy it to your home directory with the <code>Copy Template</code> button. Then a copy of that template should appear in the list with a source of <code>My Templates</code> rather than <code>System Templates</code>. You can select your copy, <code>View Files</code>, and edit the job. When you're satisfied, select <code>Create New Job</code>.</p>"},{"location":"slurm/ood.html#active-jobs","title":"Active Jobs","text":"<p>You can view information on your jobs, view the output files from your jobs, and kill your jobs from the Active Jobs page. Open it by clicking <code>Jobs &gt; Active Jobs</code> or by following this link</p> <p></p> <p>It should change your tab to look like this:</p> <p></p>"},{"location":"slurm/ood.html#_1","title":"OnDemand","text":""},{"location":"slurm/ood.html#_2","title":"OnDemand","text":""},{"location":"slurm/sbatch.html","title":"Using SBATCH to Submit Jobs","text":"<p>SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using <code>squeue</code> while it is pending or running and <code>sacct</code> at any time.</p> <pre><code>squeue --me\nsacct -j YOUR_JOBID\n</code></pre> <p>SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than <code>sbatch &lt;batch file&gt;</code>, because you can specify all parameters in the command inside the file itself.</p> <p>The following is an example of a batch script. Please note that the top of the script must start with <code>#!/bin/bash</code> (or whatever interpreter you need, if you don't know, use bash), and then immediately follow with <code>#SBATCH &lt;param&gt;</code> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition.</p> <pre><code>#!/bin/bash\n#SBATCH -c 4  # Number of Cores per Task\n#SBATCH --mem=8192  # Requested Memory\n#SBATCH -p gpu  # Partition\n#SBATCH -G 1  # Number of GPUs\n#SBATCH -t 01:00:00  # Job time limit\n#SBATCH -o slurm-%j.out  # %j = job ID\n\nmodule load cuda/10\n/modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery\n</code></pre> <p>This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.</p> <p>If you need to run the same type of job over many inputs, different parameters, or even just some number of iterations, you should look into using Job Arrays to simplify your workflow.</p>"},{"location":"slurm/sbatch.html#email","title":"Email","text":"<p>Slurm can send you emails based on the status of your job via the <code>--mail-type</code> argument.</p> <p>Common mail types are <code>BEGIN, END, FAIL, INVALID_DEPEND, and REQUEUE</code>. See the sbatch man page</p> <p>Example:</p> <pre><code>srun --mail-type=BEGIN hostname\n</code></pre> <p>or:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-type=BEGIN\nhostname\n</code></pre> <p>There is also the <code>--mail-user</code> argument, but this is optional. Our mail server knows the email you used to register your Unity account.</p>"},{"location":"slurm/sbatch.html#time-limit-email-preventing-loss-of-work","title":"Time Limit Email - Preventing Loss of Work","text":"<p>When your job reaches its time limit, it will be killed, even if it's 99% of the way through its task. Without checkpointing, all those CPU hours will be for nothing and you will have to schedule the job all over again.</p> <p>One way to prevent this is to check on your job's output as it approaches its time limit. You can specify <code>--mail-type=TIME_LIMIT_80</code>, and Slurm will email you if 80% of the time limit has passed and your job is still running. Then you can check on the job's output and determine if it will finish in time. If you think that your job will not finish in time, you can email us at hpc@umass.edu and we can extend your job's time limit.</p>"},{"location":"slurm/slurm-mail.html","title":"Slurm mail","text":""},{"location":"slurm/slurm-mail.html#email","title":"Email","text":"<p>Slurm can send you emails based on the status of your job via the <code>--mail-type</code> argument.</p> <p>Common mail types are <code>BEGIN, END, FAIL, INVALID_DEPEND, and REQUEUE</code>. See the sbatch man page</p> <p>Example:</p> <pre><code>srun --mail-type=BEGIN hostname\n</code></pre> <p>or:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-type=BEGIN\nhostname\n</code></pre> <p>There is also the <code>--mail-user</code> argument, but this is optional. Our mail server knows the email you used to register your Unity account.</p>"},{"location":"slurm/slurm-mail.html#time-limit-email-preventing-loss-of-work","title":"Time Limit Email - Preventing Loss of Work","text":"<p>When your job reaches its time limit, it will be killed, even if it's 99% of the way through its task. Without checkpointing, all those CPU hours will be for nothing and you will have to schedule the job all over again.</p> <p>One way to prevent this is to check on your job's output as it approaches its time limit. You can specify <code>--mail-type=TIME_LIMIT_80</code>, and Slurm will email you if 80% of the time limit has passed and your job is still running. Then you can check on the job's output and determine if it will finish in time. If you think that your job will not finish in time, you can email us at hpc@umass.edu and we can extend your job's time limit.</p>"},{"location":"slurm/srun.html","title":"Using SRUN to Submit Jobs","text":"<p>Note</p> <p>Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below).</p> <p>SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run <code>srun /bin/hostname</code> and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue.</p> <p>Please note that like sbatch, you can run a batch file using srun.</p> <p>The command syntax is <code>srun &lt;options&gt; [executable] &lt;args&gt;</code></p> <p>Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run <code>man srun</code>.</p> <ul> <li><code>-c &lt;num&gt;</code> Number of CPUs (threads) to allocate to the job per task</li> <li><code>-n &lt;num&gt;</code> The number of tasks to allocate (for MPI)</li> <li><code>-G &lt;num&gt;</code> Number of GPUs to allocate to the job</li> <li><code>--mem &lt;num&gt;[K|M|G|T]</code> Memory to allocate to the job (in MB by default)</li> <li><code>-p &lt;partition&gt;</code> Partition to submit the job to</li> </ul> <p>To run an interactive job (in this case a bash prompt), the command might look like this (<code>--pty</code> is the important option):</p> <pre><code>srun -c 6 -p cpu --pty bash\n</code></pre> <p>To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the <code>--x11</code> argument:</p> <pre><code>srun -c 6 -p cpu --pty --x11 xclock\n</code></pre> <p>Note</p> <p>You cannot run an interactive/gui job using the <code>sbatch</code> command, you must use <code>srun</code>.</p>"},{"location":"slurm/srun.html#email","title":"Email","text":"<p>Slurm can send you emails based on the status of your job via the <code>--mail-type</code> argument.</p> <p>Common mail types are <code>BEGIN, END, FAIL, INVALID_DEPEND, and REQUEUE</code>. See the sbatch man page</p> <p>Example:</p> <pre><code>srun --mail-type=BEGIN hostname\n</code></pre> <p>or:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-type=BEGIN\nhostname\n</code></pre> <p>There is also the <code>--mail-user</code> argument, but this is optional. Our mail server knows the email you used to register your Unity account.</p>"},{"location":"slurm/srun.html#time-limit-email-preventing-loss-of-work","title":"Time Limit Email - Preventing Loss of Work","text":"<p>When your job reaches its time limit, it will be killed, even if it's 99% of the way through its task. Without checkpointing, all those CPU hours will be for nothing and you will have to schedule the job all over again.</p> <p>One way to prevent this is to check on your job's output as it approaches its time limit. You can specify <code>--mail-type=TIME_LIMIT_80</code>, and Slurm will email you if 80% of the time limit has passed and your job is still running. Then you can check on the job's output and determine if it will finish in time. If you think that your job will not finish in time, you can email us at hpc@umass.edu and we can extend your job's time limit.</p>"},{"location":"software/index.html","title":"Unity Software Overview","text":""},{"location":"software/index.html#means-of-installing-packages","title":"Means of installing packages","text":""},{"location":"software/index.html#apt-package-manager","title":"<code>apt</code> package manager","text":"<p>The Ubuntu system package manager <code>apt</code> downloads its packages pre-compiled from the Ubuntu repository. These are placed in standard locations like <code>/usr/bin</code> so that they are always found in your <code>$PATH</code>. This can only be done by administrators. To avoid conflicts, most software is only available in one version. These packages will change with an (inevitable) operating system update. The admin team is trying to avoid <code>apt</code> installs for research computing software.</p> <p>Relevant Documentation:</p> <ul> <li>None</li> </ul>"},{"location":"software/index.html#environment-modules","title":"Environment Modules","text":"<p>There are a wide variety of modules available with the <code>module</code> command. Most software requests are fulfilled here. These are compiled by the admins and are stored in <code>/modules/apps/</code> or in <code>/modules/spack/opt/spack/</code>.</p> <p>Relevant Documentation:</p> <ul> <li>intro to environment modules</li> <li>using environment modules</li> <li>module hierarchy</li> </ul>"},{"location":"software/index.html#conda","title":"Conda","text":"<p>The conda package manager allows users to compile software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules.</p> <p>Relevant Documentation:</p> <ul> <li>conda</li> </ul>"},{"location":"software/index.html#r-package-manager","title":"R package manager","text":"<p>Coming soon!</p>"},{"location":"software/index.html#docker-singularity","title":"Docker (Singularity)","text":"<p>Coming soon!</p>"},{"location":"software/apptainer.html","title":"Containers","text":"<p>Containers allow you to disregard all of the software installed on a given machine, and instead run a completely different software stack off the same kernel. This means you can run Debian, CentOS, Arch Linux, all on on Unity. Containers run your software in an isolated environment, which avoids unwanted interference and also makes your software very consistent and reproducible.</p> <p>If you're familiar with containers, then you've probably heard of Docker. The problem with Docker is that it requires root permissions, which makes it un-suitable for HPC. Apptainer works similarly to Docker but it manages to build and run containers without root access.</p> <p>To use Apptainer you will need to first load the Apptainer module:</p> <pre><code>module load apptainer/1.1.5\n</code></pre>"},{"location":"software/apptainer.html#building-container-images","title":"Building container images","text":""},{"location":"software/apptainer.html#pull-a-container-from-online","title":"Pull a container from Online","text":"<p>You can download and a recipe from online and build it into an image in one simple command:</p> <pre><code>apptainer pull &lt;output-image-file&gt; &lt;container-location&gt;\napptainer pull lolcow.sif docker://sylabsio/lolcow:latest\n</code></pre>"},{"location":"software/apptainer.html#write-your-own-recipe","title":"Write your own Recipe","text":"<p>You might want to modify a container you found online. Rather than making your image into a sandbox and editing the files by hand, best practice would be to write your modifications into a recipe. This makes your software environment reproducible.</p> <p>Here's an example of a simple recipe <code>train.def</code>:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:focal\n\n%post\napt update &amp;&amp; apt install -y sl\n\n%runscript\n/usr/games/sl\n</code></pre> <p>Note</p> <p>Apptainer requires a recipe in the form of a definition (.def) file. You can convert a dockerfile to a .def file using Singularity Python:</p> <p><code>module load spython</code></p> <p><code>spython recipe ./my-dockerfile &gt; my-def-file</code></p> <p>Once your recipe is complete, build it:</p> <pre><code>apptainer build &lt;output-image-file&gt; &lt;recipe-path&gt;\napptainer build train.sif train.def\n</code></pre> <p>Note</p> <p>Sometimes a build will fail with an error related to bind-mounting <code>/work</code>. If mounting <code>/work</code> at build-time is not required, then you can just edit <code>APPTAINER_BINDPATH</code> so that Apptainer will no longer try to mount it.</p> <p>Useful arguments:</p> <ul> <li>bind mount (<code>-B --bind</code>): make a directory on Unity's filesystem available inside the container. This is a comma-delimited list of directory mappings of the form: <code>source:destination</code>. Other options are also available, see the documentation.<ul> <li>This is also determined by environment variable <code>APPTAINER_BINDPATH</code>, which has a default value: <code>/work,/home,/nese,/project,/gypsum,/scratch,/modules,/nas,/datasets</code></li> </ul> </li> <li>Docker login (<code>--docker-login</code>): login to a Docker repository.</li> <li>sandbox (<code>-s --sandbox</code>): build the container in a writable directory, rather than a .sif image.</li> </ul> <p>Learn More:</p> <ul> <li>Apptainer docs: pull</li> <li>Apptainer docs: definition file</li> <li>Apptainer docs: build</li> </ul>"},{"location":"software/apptainer.html#using-container-images","title":"Using container images","text":"<p>Given an Apptainer image (<code>.sif</code> or sandbox directory), you can:</p> <ul> <li>run the recipe's <code>runscript</code> with <code>apptainer run &lt;image-file&gt; &lt;optional runscript arguments&gt;</code></li> <li>start a shell with <code>apptainer shell &lt;image-file&gt;</code></li> <li>run a specific command with <code>apptainer exec &lt;image-file&gt; &lt;command&gt;</code></li> </ul> <p>Useful arguments:</p> <ul> <li>nvidia (<code>--nv</code>): enable Nvidia GPU support</li> <li>bind mount (<code>-B --bind</code>): see description above</li> <li>clean environment (<code>-e --cleanenv</code>): don't export any environment variables into the container.</li> <li>fake root access (<code>-f --fakeroot</code>): simulate root access inside the container.</li> </ul> <p>Learn More:</p> <ul> <li>Apptainer docs: run</li> <li>Apptainer docs: exec</li> <li>Apptainer docs: shell</li> </ul>"},{"location":"software/conda.html","title":"Using Conda Environments","text":""},{"location":"software/conda.html#introduction","title":"Introduction","text":"<p>The conda package manager allows users to install software easily and without admin privileges. Conda environments can be created for any software set, and can be enabled/disabled dynamically not unlike modules.</p> <p>A conda environment is not to be confused with the environment of your login shell. The package tied to an environment module is compiled by hand by the Unity admins, where conda packages can be installed by any user with a simple command. A conda environment can contain any number of packages, where a module usually only contains one. Modules and conda environments can be used together.</p> <p>On Unity we use Miniconda, as opposed to Anaconda. From a user's perspective they can be considered to be the same thing.</p> <p>Note</p> <p>When working on a conda environment, make sure you activate it! Without a currently active environment, conda will attempt to modify the global Unity environment, and you will get <code>permission denied</code>.</p>"},{"location":"software/conda.html#setup","title":"Setup","text":"<p>The <code>conda</code> command is not available unless the <code>miniconda</code> module is loaded.</p> <p>If you see this:</p> <pre><code>conda: Command not found\n</code></pre> <p>Do this:</p> <pre><code>module load miniconda\n</code></pre>"},{"location":"software/conda.html#creating-an-environment","title":"Creating an Environment","text":"<p>You can create as many conda environments as you desire, limited only by our disk quotas.</p> <pre><code>conda create --name testName python=3.7\n</code></pre> <p>This creates an environment in your home folder, specifically <code>/home/$USER/.conda/envs/&lt;name&gt;</code>.</p> <p>You can also create environments in other directories, such as your PI's work directory.</p> <pre><code>mkdir -p /work/pi_name/$USER-conda/envs\nconda create --prefix /work/pi_name/$USER-conda/envs/testName python=3.7\n\n# OPTIONAL, make a symlink (shortcut) to home directory\nln -s /work/pi_name/$USER-conda/envs/testName ~/testName\n</code></pre> <p>Replace <code>testName</code> with the name of your choice, and replace <code>3.7</code> with your Python version of choice.</p> <p>Note</p> <p>The <code>$USER</code> environment variable evaluates to your username.</p>"},{"location":"software/conda.html#activating-an-environment","title":"Activating an Environment","text":"<p>Environment created with <code>--name</code>:</p> <pre><code>conda activate testName\n</code></pre> <p>Environment created with <code>--prefix</code>:</p> <pre><code>conda activate /work/pi_name/$USER-conda/envs/testName\n# OR\ncd /work/pi_name/$USER-conda/envs/\nconda activate ./testName\n</code></pre> <p>Your currently active conda environment will appear in parentheses to the left of your command line prompt:</p> <pre><code>user@login2:~$ conda activate ./testName\n(testName) user@login2:~$\n</code></pre>"},{"location":"software/conda.html#adding-packages-to-your-environment","title":"Adding Packages to your Environment","text":"<pre><code>conda install numpy\n</code></pre> <p>The install will ask you to confirm installing numpy as well as any other additional required packages.</p>"},{"location":"software/conda.html#list-available-environments","title":"List Available Environments","text":"<pre><code>conda env list\n</code></pre>"},{"location":"software/conda.html#list-packages-installed-in-the-current-environment","title":"List Packages Installed in the Current Environment","text":"<pre><code>conda list\n</code></pre>"},{"location":"software/conda.html#delete-an-environment","title":"Delete an Environment","text":"<pre><code>conda remove --name testName --all\n</code></pre> <p>If your environment was added to JupyterHub, you will have to remove it manually.</p> <pre><code>rm -rf ~/.local/share/jupyter/kernels/testName\n</code></pre>"},{"location":"software/conda.html#conda-environments-and-jupyter","title":"Conda Environments and Jupyter","text":"<p>You can create many custom conda environments and use them within JupyterHub. This must be done in the command line, but JupyterHub provides a command line interface in it's 'Terminal' app.</p>"},{"location":"software/conda.html#adding-your-environment-to-jupyterhub","title":"Adding your Environment to JupyterHub","text":"<p>Note</p> <p>make sure your environment is activated first. Without a currently active environment, conda will attempt to modify the main default environment, and you will get permission denied.</p> <pre><code>conda install ipykernel\n</code></pre> <p>Add a kernelspec (Kernel Specification) to your JupyterHub.</p> <pre><code>python -m ipykernel install --user --name testName --display-name=\"Display Name Within JupyterHub\"\n</code></pre> <p>If the above was done within JupyterHub, reload the page. If that doesn't work, restart your JupyterHub server.</p>"},{"location":"software/conda.html#learn-more","title":"Learn more","text":"<p>Conda documentation</p>"},{"location":"software/module-hierarchy.html","title":"Module Hierarchy","text":"<p>Environment Modules is a tool to change dynamically what software is available for use by a given user at a given time. Before you read this, it's recommended that you first read the  introduction and the module usage guide.</p> <p>As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. Our strategy is to create a module hierarchy to divide modules according to their stacks. This makes it much more difficult to accidentally load modules which are incompatible with each other.</p> <p>The <code>$MODULEPATH</code> environment variable is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default.</p> <p>This means not all modules can be found with <code>module avail</code> by default.</p>"},{"location":"software/module-hierarchy.html#here-is-the-full-unity-module-hierarchy-as-of-2023415","title":"Here is the full Unity module hierarchy as of 2023/4/15:","text":"<p>Compilers are red, and providers are blue.</p> <pre><code>/modules/modulefiles/\n\n/modules/spack_modulefiles/\n\u251c\u2500\u2500 linux-ubuntu20.04-x86_64\n|\u00a0\u00a0 \u251c\u2500\u2500 Core\n|\u00a0\u00a0 \u251c\u2500\u2500 intel\n|\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 2021.4.0\n|\u00a0\u00a0 \u251c\u2500\u2500 atlas\n|\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 3.10.3-sfhhdph\n|\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 Core\n|\u00a0\u00a0 \u251c\u2500\u2500 intel-oneapi-mpi\n|\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 2021.6.0-h3cppyo\n|\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 Core\n|\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 openblas\n|\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 0.3.18-6pbqv7b\n|\u00a0\u00a0 \u2502\u00a0\u00a0             \u2514\u2500\u2500 Core\n|\u00a0\u00a0 \u251c\u2500\u2500 openblas\n|\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 0.3.18-6pbqv7b\n|\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 Core\n|\u00a0\u00a0 \u2514\u2500\u2500 openmpi\n|\u00a0\u00a0     \u251c\u2500\u2500 4.1.3-3rgk3nu\n|\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Core\n|\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 intel-mkl\n|\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 2020.4.304-gmusbfh\n|\u00a0\u00a0     \u2502\u00a0\u00a0         \u2514\u2500\u2500 Core\n|\u00a0\u00a0     \u2514\u2500\u2500 4.1.4-tauaqk4\n|\u00a0\u00a0         \u251c\u2500\u2500 Core\n|\u00a0\u00a0         \u2514\u2500\u2500 intel-mkl\n|\u00a0\u00a0             \u2514\u2500\u2500 2020.4.304-gmusbfh\n|\u00a0\u00a0                 \u2514\u2500\u2500 Core\n\u251c\u2500\u2500 linux-ubuntu20.04-aarch64\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Core\n\u2514\u2500\u2500 linux-ubuntu20.04-ppc64le\n \u00a0\u00a0 \u251c\u2500\u2500 Core\n \u00a0\u00a0 \u251c\u2500\u2500 openblas\n \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 0.3.21-coxg6gz\n \u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 Core\n \u00a0\u00a0 \u2514\u2500\u2500 openmpi\n \u00a0\u00a0     \u251c\u2500\u2500 4.1.3-edoxxdf\n \u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Core\n \u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 xl\n \u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 16.1\n \u00a0\u00a0     \u2514\u2500\u2500 4.1.4-476r55m\n \u00a0\u00a0         \u2514\u2500\u2500 Core\n\n</code></pre> <p>Note</p> <p>Random characters at the end of compiler/provider version numbers can usually be ignored.</p> <p><code>Core</code> refers to modules compiled with Ubuntu's default GNU compiler suite, and without any special providers. The majority of Unity's modules are found here.</p> <p><code>intel</code> refers to the classic intel compilers (<code>icc</code>, <code>ifort</code>, <code>icpc</code>, ...).</p> <p>The <code>intel-oneapi-compilers-classic</code> module adds <code>intel</code> to modulepath.</p>"},{"location":"software/module-hierarchy.html#hierarchy-naming-scheme","title":"Hierarchy naming scheme","text":"<pre><code>linux-ubuntu20.04-[architecture]/[compiler]/[name]/[version]\nlinux-ubuntu20.04-[architecture]/[provider]/[compiler]/[name]/[version]\nlinux-ubuntu20.04-[architecture]/[provider]/[another-provider]/[compiler]/[name]/[version]\n</code></pre> <ul> <li>In this naming scheme, <code>Core</code> counts as a compiler.</li> </ul>"},{"location":"software/module-hierarchy.html#how-to-use-the-hierarchy","title":"How to use the hierarchy","text":"<p>You can find modules anywhere in the hierarchy with the <code>unity-module-find</code> command.</p> <p>From the full path of your desired module you should be able to tell which other modules need to be loaded first.</p> <p>Note</p> <p><code>Core</code> is always automatically added to <code>$MODULEPATH</code>!</p>"},{"location":"software/module-hierarchy.html#example","title":"Example:","text":"<pre><code>user@login1:~$ module load gromacs/2021.3\nNo module(s) or extension(s) found!\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <pre><code>user@login1:~$ unity-module-find gromacs\nModules found:\nlinux-ubuntu20.04-x86_64/intel-oneapi-mpi/2021.6.0-h3cppyo/Core/gromacs/2021.3\nlinux-ubuntu20.04-x86_64/openmpi/4.1.3-3rgk3nu/intel-mkl/2020.4.304-gmusbfh/Core/gromacs/2021.3\n</code></pre> <p>This is the module that I want:</p> <pre><code>linux-ubuntu20.04-x86_64/openmpi/4.1.3-3rgk3nu/intel-mkl/2020.4.304-gmusbfh/Core/gromacs/2021.3\n</code></pre> <p>In that path I can see which modules must loaded first. The new command becomes:</p> <pre><code>$ module load openmpi/4.1.3 intel-mkl/2020.4.304 gromacs/2021.3\n</code></pre>"},{"location":"software/module-hierarchy.html#learn-more","title":"Learn more","text":"<p>About the Hierarchy Change</p> <p>https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy</p> <p>https://lmod.readthedocs.io/en/latest/080_hierarchy.html</p>"},{"location":"software/module-intro.html","title":"Intro. to Environment Modules","text":"<p>As a Unity user, you have access to a wide variety of software. Making all of this software available simultaneously and without conflicts is a complex problem, and the solution is Environment modules.</p> <p>An environment is a set of shell variables of the form <code>KEY=\"VALUE\"</code>.</p> <p>Note</p> <p>You can see your current environment using the <code>env</code> command.</p>"},{"location":"software/module-intro.html#the-path-environment-variable","title":"The <code>PATH</code> environment variable","text":"<p><code>$PATH</code> is a list of directories (folders) delimited by colons.</p> <pre><code>$ echo $PATH\n/usr/local/bin:/usr/bin:/bin\n</code></pre> <p>Most of the commands you use in the shell are actually executable files somewhere on the filesystem. When you enter a command, the shell searches the directories in <code>$PATH</code> (from left to right) for an executable by that name. If there are multiple executables of the same name, whichever is found earlier in the <code>$PATH</code> (further to the left) is used.</p> <p>This means that the commands available in your shell can be changed by changing your environment variables.</p>"},{"location":"software/module-intro.html#modules","title":"Modules","text":"<p>Environment modules are scripts that modify your environment. We use modules to add new directories to your <code>$PATH</code>, making the executables within available for use. We 'prepend' the <code>$PATH</code>, making this new directory furthest to the left. This makes sure that the executables within are chosen first by the shell when you call their name.</p> <pre><code>$ which python3\n/usr/bin/python3\n$ module load python/3.9.1\n$ which python3\n/modules/apps/python/3.9.1/bin/python3\n</code></pre> <pre><code>$ echo $PATH\n/usr/local/bin:/usr/bin:/bin\n$ module load python/3.9.1\n$ echo $PATH\n/modules/apps/python/3.9.1/bin:/usr/local/bin:/usr/bin:/bin\n</code></pre> <p>Note</p> <p>The scripts that modify your environment are called \"modules\", \"modulefiles\", or \"environment modules\". Our module system as a whole is called \"Lmod\". There is another module system out there called \"Tmod\" or \"Environment Modules\", which Lmod is based off of. These names can be confusing.</p>"},{"location":"software/module-intro.html#learn-more","title":"Learn More","text":"<p>the <code>PATH</code> Environment Variable</p> <p>Lmod documentation</p>"},{"location":"software/module-usage.html","title":"Using Environment Modules","text":"<p>Modules are easy to use. You can <code>load</code> and <code>unload</code> them as you please, enabling and disabling different software. You can list currently active modules with <code>module list</code>, search for modules with <code>module avail</code>, and unload all active modules with <code>module purge</code>.</p> <p>While the modules work on the login nodes, the login nodes have strict CPU and memory limits. Jobs that do heavy lifting should always be scheduled through Slurm.</p>"},{"location":"software/module-usage.html#list-all-available-modules","title":"List All Available Modules","text":"<p>These all do the same thing:</p> <pre><code>module available\nmodule avail\nmodule av\nml av\n</code></pre> <p>This will return an output that looks something like this:</p> <pre><code>------------------------------------------------- /modules/modulefiles -------------------------------------------------\n   R/3.6.2             cuda/10.1.243 (D)    gcc/9.2.0               julia/1.1.1         openmpi/4.0.4\n   cmake/3.7.2         cuda/11.0.1          glxgears/1.0            jupyter/3.6.8       python/2.7.16\n   cmake/3.15.0 (D)    fd3dspher/1.0        gmsh/4.4.1              mathematica/12.0    python/3.7.4  (D)\n   cuda/8.0.61         gcc/5.5.0            gpu-burn/default        mesa/19.0.8         qt/5.13.1\n   cuda/9.0.176        gcc/6.5.0            gromacs/2020.2C         miniconda/3.7       stress/1.0.4\n   cuda/9.2.148        gcc/7.4.0     (D)    gromacs/2020.2G  (D)    opencl/2.2.11       vtk/8.2.0\n</code></pre> <p>Note</p> <p>The module list is rapidly growing. To see the full list, use this command in your terminal.</p>"},{"location":"software/module-usage.html#search-for-modules","title":"Search for Modules","text":"<pre><code>module avail gcc\n</code></pre> <p>This filters the output of <code>module avail</code> for just the <code>gcc</code> modules.</p>"},{"location":"software/module-usage.html#loading-modules","title":"Loading Modules","text":"<pre><code>module load gcc/9.2.0\n</code></pre> <p>Note</p> <p>Not all modules are shown by default in <code>module avail</code>. See module hierarchy.</p>"},{"location":"software/module-usage.html#unloading-modules","title":"Unloading Modules","text":"<pre><code>module unload gcc\n</code></pre>"},{"location":"software/module-usage.html#unloading-all-modules","title":"Unloading All Modules","text":"<pre><code>module purge\n</code></pre>"},{"location":"software/module-usage.html#list-currently-loaded-modules","title":"List Currently Loaded Modules","text":"<pre><code>module list\n</code></pre>"},{"location":"software/ood.html","title":"GUI Apps via OnDemand","text":"<p>Unity OnDemand provides \"Batch Connect\", which starts your software as a batch job, then connects you to your software via a VNC server. Anything that you could have run via X11 forwarding can now be run much faster and more easily through Batch Connect.</p> <p>As of Jan. 2023, we provide JupyterLab, Matlab, Rstudio, Mathematica, and a XFCE, a general-purpose interactive desktop environment.</p> <p>You can begin by clicking on <code>Interactive Sessions</code> or by following this link.</p> <p></p> <p>It should change your tab to look like this:</p> <p></p> <p>You can select an app from the list on the left. This should open a form. Select the resources you would like to schedule (CPU threads, time, GPU's) and leave any fields blank that you don't understand or don't care about. Then click <code>Launch</code>.</p> <p>This should return you to <code>My Interactive Sessions</code>. Here, you will see your job waiting in the queue.</p> <p></p> <p>When it has left the queue, it will turn from <code>Queued</code> to <code>Starting</code> to <code>Running</code>, and a blue button should appear at the bottom of the job card. Click it to connect to your job.</p> <p></p>"},{"location":"software/side-notes.html","title":"Side notes","text":"<p>An environment can also include aliases and functions. These can shortcut the previously mentioned process of <code>$PATH</code> searching.</p> <p>Note</p> <p>You can list all shell builtins with the <code>compgen -b</code> command.</p> <p>You can list all functions with the <code>declare -F</code> command.</p> <p>You can list all aliases with the <code>alias</code> command.</p> <p>You can see the definition of a function or alias with the <code>type</code> command.</p> <p>Note</p> <p><code>ml</code> is a shortcut. On its own it means <code>module list</code>. Combined with the name of a module it means <code>module load</code>. Combined with other <code>module</code> commands it simply means <code>module</code>.</p>"},{"location":"technical/nodelist.html","title":"Node List","text":"<p>The Unity cluster is a heterogeneous cluster. We plan to keep it a heterogeneous cluster. To clear confusion, a node list is below, with slurm constraints to refine your node selection.</p> <p>You can use the <code>-C</code> flag in your slurm jobs to batch your jobs to a specific set of nodes. If you don't include a constraint, your job may land in any number of nodes in the partition.</p> <p>You can see a list of all possible constraints with the <code>unity-slurm-list-constraints</code> command.</p> <p>You can see a list of nodes that meet a given constraint with the <code>unity-slurm-find-nodes</code> command.</p>"},{"location":"technical/nodelist.html#cpu-nodes","title":"CPU Nodes","text":"Name Model CPU (Cores and Threads Per CPU) RAM Partitions Constraints cpu[001-008] Lenovo ThinkSystem SD530 2x Intel Xeon Gold 6126 (24 Cores per node) 192 GiB (Node(s) 15-8) 384 GiB (Nodes 2-4) <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>len-sd530_2018</li><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-skylake_avx512</li></ul> cpu[013-025] Dell Poweredge R640 2x Intel Xeon Gold 6148 (40 Cores per node) 192 GiB <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>dell-r640_2020&lt;/li<li>avx512</li><li>intel</li><li>linux-ubuntu20.04-skylake_avx512</li> cpu[022-030] Dell Poweredge R6525 2x AMD Epyc 7763 (128 Cores per node) 984GiB <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>dell_r6525</li><li>amd</li><li>zen3</li><li>x86_64_v3</li><li>x86_64</li></ul> cpu[031-044] Dell Poweredge R7525 2x AMD Epyc 7543 (64 Cores per node) 512GiB <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>amd</li><li>zen3</li><li>x86_64_v3</li><li>x86_64</li></ul> ceewater-cpu[001-007] Lenovo ThinkSystem SR635 1x AMD EPYC-Rome 7402 (12 Cores per node) 128 GiB <ul><li>ceewater_cjgleason-cpu</li><li>ceewater_casey-cpu</li><li>ceewater_kandread-cpu</li><li>cpu-preempt</li></ul> <ul><li>ceewater_len-sr635_2020</li><li>amd</li><li>linux-ubuntu20.04-zen2</li></ul> astroth-cpu[001-008] SuperMicro SBI-4429P 2x Xeon Silver 4215R (16 Cores per node) 192 GiB <ul><li>astroth-cpu</li><li>cpu-preempt</li></ul> <ul><li>astroth_smicro-sbi4429p_2021</li><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-cascadelake</li></ul> zhoulin-cpu[001-006] Lenovo SR645 2x AMD EPYC 7702 (128 Cores per node) 512 GiB <ul><li>zhoulin-cpu</li><li>cpu-preempt</li></ul> <ul><li>zhoulin_len-sr645_2021</li><li>amd</li><li>linux-ubuntu20.04-zen2</li></ul> toltec-cpu[001-006] Dell R640 2x Intel Xeon Gold 5218 CPU (32 Cores per node) 383 GiB <ul><li>toltec-cpu</li><li>cpu-preempt</li></ul> <ul><li>toltec_dell-r640_2021</li><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-cascadelake</li></ul> gaoseismolab-cpu[001-005] Lenovo SR630 v2 2x Intel Xeon Platinum 8358 (64 Cores per node) 512 GiB <ul><li>gaoseismolab-cpu</li><li>cpu-preempt</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li></ul> uri-cpu[001-005] Intel S2600BPB 2x Intel Xeon Gold 6238R (56 Cores per node) 512 GiB <ul><li>uri-cpu</li><li>cpu</li><li>cpu-long</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-cascadelake</li></ul> uri-cpu[006-021] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 128 GiB <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li></ul> uri-cpu[022-037] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 128 GiB <ul><li>cpu-preempt</li><li>uri-cpu</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li></ul> uri-cpu[038-041] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 1024 GiB <ul><li>cpu-preempt</li><li>uri-cpu</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li></ul> uri-cpu[041-045] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 1024 GiB <ul><li>cpu</li><li>cpu-long</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li></ul> uri-cpu[046-047] PowerNV 8335-GTH POWER9 (32 Cores, 128 Threads) 256 GiB <ul><li>uri-power9</li><ul> <ul><li>power9le</li><li>altivec</li><li>ppc64le</li></ul> umd-cscdr-cpu[001-053] GIGABYTE H262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 256 GiB <ul><li>cpu-preempt</li><li>umd-cscdr-cpu</li> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li></ul> umd-cscdr-arm[001-003] GIGABYTE R152 1x Neoverse-N1 (80 Cores per node) 256 GiB <ul><li>arm-preempt</li><li>umd-cscdr-arm</li> <ul><li>arm64</li><li>aarch64</li></ul> visterra-cpu[001-008] Dell R640 2x Intel(R) Xeon(R) Gold 6248R (48 Cores per node) 376 GiB <ul><li>cpu-preempt</li><li>visterra</li></ul> <ul><li>avx512</li><li>intel</li><li>cascadelake</li><li>ib</li></ul>"},{"location":"technical/nodelist.html#gpu-nodes","title":"GPU Nodes","text":"Name Model CPU GPU RAM Partitions Constraints gpu[001-002] Lenovo ThinkSystem SR650 2x Intel Xeon Silver 4110 (16 Cores per node) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB <ul><li>gpu</li><li>gpu-long</li></ul> <ul><li>len-sr650_2018</li><li>avx512</li><li>v100</li><li>intel</li><li>linux-ubuntu20.04-skylake_avx512</li></ul> gpu[003-004] Dell Poweredge R740 2x Intel Xeon Gold 6140 (36 Cores per node) 2x NVIDIA Tesla V100 (16GB VRAM) 192 GiB <ul><li>gpu</li><li>gpu-long</li></ul> <ul><li>len-sr650_2018</li><li>avx512</li><li>v100</li><li>intel</li><li>linux-ubuntu20.04-skylake_avx512</li></ul> ials-gpu[001-033] Atipa 2x Intel Xeon Silver 4214R (24 Cores per node) 8x NVIDIA RTX 2080ti (12GB VRAM) 192 GiB <ul><li>ials-gpu</li><li>gpu</li><li>gpu-long</li></ul> <ul><li>ials_gigabyte_2020</li><li>avx512</li><li>2080ti</li><li>intel</li><li>linux-ubuntu20.04-cascadelake</li></ul> astroth-gpu[001-003] ASRock AMD Ryzen Threadripper 1900X (8 Cores per node) 2x NVIDIA RTX 2080 (8GB VRAM) 32 GiB <ul><li>astroth-gpu</li></ul><li>gpu-preempt</li> <ul><li>astro_asrock_x399_2020</li><li>2080</li><li>amd</li><li>linux-ubuntu20.04-zen</li></ul> ece-gpu[001-002] Lenovo SR670 2x Intel Xeon Gold 6226R CPU (32 Cores per node) 4x NVIDIA Tesla A100 (40GB VRAM) 384 GiB <ul><li>ece-gpu</li></ul><li>gpu-preempt</li> <ul><li>ece_len-sr670_2021</li><li>avx512</li><li>a100</li><li>intel</li></ul> gypsum-gpu[001-025] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores per node) 4x NVIDIA Tesla M40 (24GB VRAM) 256 GiB <ul><li>gypsum-m40</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-haswell</li></ul> gypsum-gpu[043-099] ASUSTeK ESC4000 G3 Series 2x Intel Xeon E5-2620 v3 (12 Cores per node) 4x NVIDIA GeForce GTX TITAN X (12GB VRAM) 256 GiB <ul><li>gypsum-titanx</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-haswell</li></ul> gypsum-gpu[104-156] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores per node) 8x NVIDIA GeForce GTX 1080 Ti (11GB VRAM) 384 GiB <ul><li>gypsum-1080ti</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-skylake_avx512</li></ul> gypsum-gpu[157-181] TYAN B7109F77DV14HR-2T-N 2x Intel Xeon Silver 4116 (24 Cores per node) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB <ul><li>gypsum-2080ti</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-skylake_avx512</li></ul> gypsum-gpu[182-189] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores per node) 8x NVIDIA Quadro RTX 8000 (48GB VRAM) 384 GiB <ul><li>gypsum-rtx8000</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-skylake_avx512</li></ul> gypsum-gpu[190-192] Supermicro SYS-4029GP-TRT2 2x Intel Xeon Silver 4116 (24 Cores per node) 8x NVIDIA GeForce GTX 2080 Ti (11GB VRAM) 384 GiB <ul><li>gypsum-2080ti</li><li>gpu-preempt</li></ul> <ul><li>linux-ubuntu20.04-skylake_avx512</li></ul> uri-gpu[001-004] GIGABYTE G262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 4x NVIDIA Tesla A100 (80GB VRAM) 512 GiB <ul><li>gpu</li><li>gpu-long</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li><li>a100</li></ul> uri-gpu[005-008] GIGABYTE G262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 4x NVIDIA Tesla A100 (80GB VRAM) 512 GiB <ul><li>gpu-preempt</li><li>uri-gpu</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li><li>a100</li></ul> umd-cscdr-gpu001 GIGABYTE G262 2x Intel Xeon(R) Platinum 8352Y (64 Cores per node) 4x NVIDIA Tesla A100 (80GB VRAM) 513 GiB <ul><li>gpu-preempt</li><li>umd-cscdr-gpu</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li><li>a100</li></ul> umd-cscdr-gpu002 Supermicro AS-4124GO-NART 2x AMD EPYC 7763 Milan (64 Cores per node) 8x NVIDIA Tesla A100 (80GB VRAM) 2TB <ul><li>gpu-preempt</li><li>umd-cscdr-gpu</li></ul> <ul><li>avx512</li><li>intel</li><li>linux-ubuntu20.04-icelake</li><li>ib</li><li>a100</li></ul> power9-gpu[001-006] IBM PowerSystem AC922 (PowerNV 8335-GTH) 2x Power9 (32 Cores 128 Threads per node) 2x NVIDIA Tesla V100 (32GB VRAM) 256GiB <ul><li>power9-gpu</li><li>power9-gpu-preempt</li></ul> <ul><li>power9le</li><li>ppc64le</li><li>altivec</li>v100</ul> power9-gpu[007-008] IBM PowerSystem AC922 (PowerNV 8335-GTH) 2x Power9 (32 Cores 128 Threads per node) 2x NVIDIA Tesla V100 (32GB VRAM) 256GiB <ul><li>power9-gpu-osg</li><li>power9-gpu-preempt</li></ul> <ul><li>power9le</li><li>ppc64le</li><li>altivec</li>v100</ul> power9-gpu[009-016] IBM PowerSystem AC922 (PowerNV 8335-GTH) 2x Power9 (40 Cores 160 Threads per node) 4x NVIDIA Tesla V100 (16GB VRAM) 512GiB <ul><li>power9-gpu</li><li>power9-gpu-preempt</li></ul> <ul><li>power9le</li><li>ppc64le</li><li>altivec</li>v100</ul>"},{"location":"technical/partitionlist.html","title":"Partition List","text":""},{"location":"technical/partitionlist.html#core-limits","title":"Core Limits","text":"<p>There is currently a 300 CPU core, 64 GPU limit to be shared by the users of each lab.</p> <p>When you try to go over this limit, you will be denied for <code>MaxCpuPerAccount</code>.</p>"},{"location":"technical/partitionlist.html#general-use-partitions","title":"General Use Partitions","text":"<p>These are the only partitions that are open to use by all users of Unity.</p> Name Relative Wait Time Default Job Time Time Limit Max CPU's Per Node cpu medium 1 hour 1 day 40 cpu-long long 2 days 14 days 40 cpu-preempt short 1 hour (see below) 14 days (see below) 256 gpu medium 1 hour 1 day 72 gpu-long long 2 days 14 days 72 gpu-preempt short 1 hour (see below) 14 days (see below) 64 power9 short 1 hour 30 days 128 power9-gpu medium 1 hour 30 days 128 power9-gpu-preempt short 1 hour 14 days 128 arm-preempt short 1 hour 14 days 80"},{"location":"technical/partitionlist.html#preempt","title":"Preempt","text":"<p>Nodes in preempt partitions are buy-in nodes which would normally be inaccessible to general users. Buyers in reserve the right to kill (preempt) any job running on their nodes to make room for their own jobs. When you schedule a job on a preempt partition, your job can be killed after two hours.</p> <p>If your job is preempted, it will be re-queued and start again on some other hardware. If you don't want your job re-queued (but still killed), you can use the <code>--no-requeue</code> Slurm argument. To avoid losing progress, it's a good idea to use software that supports checkpointing. Checkpointing software periodically saves its state to a file. In the event of an interruption, it can read from this file and resume work where it left off.</p>"},{"location":"technical/partitionlist.html#gypsum-cluster-partitions","title":"Gypsum Cluster Partitions","text":"<p>Gypsum users, depending on the type, have access to these partitions.</p> Name Time Limit Max GPUs - Other Max GPUs - MS Max GPUs - PhD Comments gypsum-m40 7 days 1 6 12 M40 GPU partition gypsum-titanx 7 days 4 20 40 TITAN X GPU partition gypsum-1080ti 7 days 4 20 40 1080 Ti GPU partition gypsum-2080ti 7 days 2 10 20 2080 Ti GPU partition gypsum-rtx8000 7 days 0 3 6 RTX 8000 GPU partition"},{"location":"technical/partitionlist.html#ials-cluster-partitions","title":"IALS Cluster Partitions","text":"<p>IALS members have access to these partitions.</p> Name Time Limit Comments ials-gpu 14 days GPU partition for IALS"},{"location":"technical/partitionlist.html#other-priority-partitions","title":"Other Priority Partitions","text":"<p>These partitions represent the specific labs who \"bought in\" to Unity by providing their own nodes.</p> Name Time Limit Comments ceewater_cjgleason-cpu Unlimited ceewater CPU partition for cjgleason group ceewater_casey-cpu Unlimited ceewater CPU partition for casey group ceewater_kandread-cpu Unlimited ceewater CPU partition for kandread group astroth-cpu Unlimited astroth CPU partition zhoulin-cpu Unlimited zhoulin CPU partition toltec-cpu Unlimited toltec CPU partition gaoseismolab-cpu Unlimited gaoseismolab CPU partition uri-cpu 30 days URI CPU partition uri-gpu 30 days URI GPU partition ece-gpu 5 days ece partition for ECE courses only umd-cscdr-cpu 30 days UMD CSCDR CPU partition umd-cscdr-gpu 30 days UMD CSCDR GPU partition umd-cscdr-arm 30 days UMD CSCDR ARM partition power9-gpu-osg 30 days OSG Resource Contribution"},{"location":"technical/storage.html","title":"Storage","text":"<p>Below is a table of all available storage on Unity.</p> Mountpoint Name Location Type Quota Description <code>/home</code> Home directories Everywhere HDD 50 GB Home directories should be used only for user init files. <code>/work/pi_</code> Work directories Everywhere SSD 1 TB Work should be used as the primary location for running cluster jobs. This is a shared folder for all users in the PI group. <code>/work/username</code>, is a legacy directory available to older users which is being phased out. <code>/project</code> Project directories Everywhere HDD As Needed Project directories are available to PI's upon request. Good for large dataset storage or any larger storage that is not directly used for job I/O. PI's should e-mail hpc@umass.edu to request. A common use case is generating job output in <code>/work</code> and copying to permanent storage in <code>/project</code> afterwards. Not for job I/O <code>/nese</code> NESE mounts Everywhere HDD/Tape Varying Legacy images available from the northeast storage exchange can be found here. Not for job I/O <code>/nas</code> Buy-in NAS mounts Everywhere Varying Varying Legacy location where the mounts for buy-in NAS hardware are located on Unity. For users who purchased storage nodes for their own use on Unity only. <code>/scratch</code> Scratch space Everywhere (Intended for Compute) SSD 40 TB / user, cleared at the end of job <code>/scratch/[nodeid]/[jobid]</code> is created when a job is started. That folder is assigned to $TMP and deleted after the job is complete. This directory is not directly available to users. <code>/gypsum</code> Gypsum devices Everywhere HDD Varying For users migrating from the Gypsum cluster to the Unity clusters, you will find all your old storage here."},{"location":"technical/storage.html#snapshots","title":"Snapshots","text":"<p>Backups are not available on the Unity cluster. There are temporary snapshots created each day at 1am UTC. Snapshots older than three days are deleted. Self-directed restores are accomplished by accessing read-only snapshots (see table below).</p> Filesystem Name Snapshot location <code>/home/&lt;username&gt;</code> Home directory <code>/snapshots/home/unity_&lt;timestamp&gt;/&lt;username&gt;</code> <code>/work/pi_&lt;pi-username&gt;</code> Work directory <code>/snapshots/work/unity_&lt;timestamp&gt;/pi_&lt;pi-username&gt;</code>"},{"location":"technical/storage.html#restore-files-from-a-snapshot","title":"Restore Files From a Snapshot","text":"<p>The following is an example of restoring a specific directory. In this example, we restore to a <code>restore</code> directory first to ensure we don't overwrite any changes.</p> <pre><code>$ mkdir ~/restore\n$ cp -a /snapshot/home/unity_2023-02-08_01_00_00_UTC/&lt;username&gt;/path/to/file/or/directory ~/restore/\n</code></pre>"},{"location":"tools/ColabFold-notebook.html","title":"ColabFold notebook","text":"<pre><code># enter protein sequence below\nquery_sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'\n</code></pre> <pre><code># choose a template\n#`none` = no template information is used. \n#`pdb70` = detect templates in pdb70. \n#`custom` - upload and search own templates (PDB or mmCIF format, see [notes below](#custom_templates))\ntemplate_mode = \"none\" #@param [\"none\", \"pdb70\",\"custom\"]\n</code></pre> <pre><code>display_images = True #@param {type:\"boolean\"}\nimport os\nimport re\nimport hashlib\nimport random\nfrom sys import version_info \npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>if template_mode == \"pdb70\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n</code></pre> <pre><code># remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n</code></pre> <pre><code># create output directory\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\njobname = 'test' #@param {type:\"string\"}\n# remove whitespaces\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\nos.makedirs(jobname, exist_ok=True)\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\n# create csv file to store sequence\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n</code></pre> <pre><code># choose number of models to use\nnum_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\nuse_amber = num_relax &gt; 0\n# template mode --&gt; `none` = no template information is used. `pdb70` = detect templates in pdb70. `custom` - upload and search own templates (PDB or mmCIF format, see [notes below](#custom_templates))\ntemplate_mode = \"none\" #@param [\"none\", \"pdb70\",\"custom\"]\nmsa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\npair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\nmodel_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\"]\nnum_recycles = \"auto\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\nrecycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\nmax_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\nnum_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\nuse_dropout = False #@param {type:\"boolean\"}\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\nsave_all = False #@param {type:\"boolean\"}\nsave_recycles = False #@param {type:\"boolean\"}\n#set dpi for image resolution\ndpi = 200 #@param {type:\"integer\"}\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n</code></pre> <pre><code># decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n  print(a3m_file)\n\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n</code></pre> <pre><code>def input_features_callback(input_features):  \n  if display_images:    \n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nif 'logging_setup' not in globals():\n    setup_logging(Path(os.path.join(jobname,\"log.txt\")))\n    logging_setup = True\n\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\nprint(model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,    \n    model_type=model_type,\n    num_models=5,\n    num_recycles=num_recycles,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2,3,4,5],\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n</code></pre> <pre><code>#@title Display 3D structure {run: \"auto\"}\nimport py3Dmol\nimport glob\nimport matplotlib.pyplot as plt\nfrom colabfold.colabfold import plot_plddt_legend\nfrom colabfold.colabfold import pymol_color_list, alphabet_list\nrank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\ncolor = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\nshow_sidechains = False #@param {type:\"boolean\"}\nshow_mainchains = False #@param {type:\"boolean\"}\n\ntag = results[\"rank\"][0][rank_num - 1]\njobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\npdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\npdb_file = glob.glob(pdb_filename)\n\ndef show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n  model_name = f\"rank_{rank_num}\"\n  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n\n  if color == \"lDDT\":\n    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n  elif color == \"rainbow\":\n    view.setStyle({'cartoon': {'color':'spectrum'}})\n  elif color == \"chain\":\n    chains = len(queries[0][1]) + 1 if is_complex else 1\n    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n\n  if show_sidechains:\n    BB = ['C','O','N']\n    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n  if show_mainchains:\n    BB = ['C','O','N','CA']\n    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n\n  view.zoomTo()\n  return view\n\nshow_pdb(rank_num, show_sidechains, show_mainchains, color).show()\nif color == \"lDDT\":\n  plot_plddt_legend().show() \n</code></pre> <pre><code>#@title Plots {run: \"auto\"}\nfrom IPython.display import display, HTML\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\npae = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\"))\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\nplddt = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(HTML(f\"\"\"\n&lt;style&gt;\n  img {{\n    float:left;\n  }}\n  .full {{\n    max-width:100%;\n  }}\n  .half {{\n    max-width:50%;\n  }}\n  @media (max-width:640px) {{\n    .half {{\n      max-width:100%;\n    }}\n  }}\n&lt;/style&gt;\n&lt;div style=\"max-width:90%; padding:2em;\"&gt;\n  &lt;h1&gt;Plots for {escape(jobname)}&lt;/h1&gt;\n  &lt;img src=\"{pae}\" class=\"full\" /&gt;\n  &lt;img src=\"{cov}\" class=\"half\" /&gt;\n  &lt;img src=\"{plddt}\" class=\"half\" /&gt;\n&lt;/div&gt;\n\"\"\"))\n</code></pre> <pre><code># close session\n%system scancel $SLURM_JOB_ID\n</code></pre>"},{"location":"tools/ColabFold.html","title":"Introduction to ColabFold on Unity","text":"<p>ColabFold [1] is a software developed to accelerate the prediction of protein 3D structures and protein complexes by integrating the fast search algorithm <code>MMSeqs2</code> with AlphaFold2 [2] or RoseTTAFold.  ColabFold is available on Unity through a Jupyter notebook or a batch script. Both methods make use of one graphics processing unit (GPU) and the AlphaFold2 AI tool. The output includes the predicted protein structure in a PDB format text file along with files to evaluate the results.  The notebook is designed to run ColabFold with one protein sequence while the batch script can be used to make predictions for multiple protein sequences at once. ColabFold on Unity is currently available in a beta version. For questions, please send an email to hpc@umass.edu. </p>"},{"location":"tools/ColabFold.html#using-a-jupyter-notebook-to-access-colabfold","title":"Using a Jupyter notebook to access ColabFold","text":"<p>Start by accessing JupyterLab using the Unity OnDemand interface.</p> <p>Click on the JupyterLab interactive app and fill out the following fields:</p> <ol> <li>The <code>Partition</code> field indicates the type of compute nodes to run your interactive session on. One of the gpu partitions should be selected to run ColabFold Jupyter notebook (gpu, gpu-long or gpu-preempt). For more information on partitions, see the partition list.</li> <li>The <code>Maximum job duration</code> field defines how long the interactive session with JupyterLab should run for. This field can be left with the default value of one hour (1:00:00) for short protein sequences but should be increased to make predictions on larger protein sequences.</li> <li>The <code>Memory (in GB)</code> field defines the amount of memory in gigabytes allocated to your interactive session. To give you an idea of how much memory you may need, 8GB is enough for a protein of 59 amino acids but 50 GB is required for a large protein of 2894 amino acids.</li> <li>The <code>GPU count</code> field is the number of GPUs allocated to your interactive session. It should be set to 1 since ColabFold only runs on a single GPU.</li> <li>The <code>Modules</code> field corresponds to a list of modules to load. The two following modules should be added (separated only by a space) to this field in order to use the GPU: cudnn/cuda11-8.4.1.50 cuda/11.4.0</li> </ol> <p>The fields <code>CPU thread count</code> and <code>Extra arguments for Slurm</code> can be left blank.</p> <p>Inside JupyterLab:</p> <ol> <li>Copy the ColabFold.ipynb notebook available at /datasets/bio/colabfold/ColabFold.ipynb on Unity to your work directory.</li> <li>Open the ColabFold.ipynb notebook.</li> <li>Choose <code>Python [conda env: colabfold]</code> for the kernel.</li> <li>Insert your protein sequence next to <code>query_sequence</code> and execute the code in the first cell (press SHIFT+ENTER or press the play button in the toolbar above).</li> <li>Run the code in the remaining cells in order to predict the protein structure with the default parameters (see Notes section below) and output plots and a visualization of the 3D structure.</li> <li>The output directory containing the results will be located in the folder where you put the ColabFold.ipynb notebook.</li> </ol>"},{"location":"tools/ColabFold.html#notes","title":"Notes:","text":"<ul> <li>ColabFold's notebook is setup to run with the following parameters that can be adjusted by the user:<ul> <li>No templates</li> <li>Number of models: 5</li> <li>Stop predictions at score 100</li> <li>Msa mode: mmseqs2_uniref_env</li> <li>Model type: alphafold2_ptm</li> </ul> </li> <li>The Jupyter notebook made available here is a modified version of the AlphaFold2_mmseqs2 notebook [3].</li> </ul>"},{"location":"tools/ColabFold.html#using-a-batch-script-to-run-colabfold","title":"Using a batch script to run ColabFold","text":"<p>The example of batch scripts provided below should be considered for users dealing with a large number of protein sequences.</p>"},{"location":"tools/ColabFold.html#colabfold-on-unity-has-3-main-components","title":"ColabFold on Unity has 3 main components:","text":"<ol> <li>A script colabfold_search that searches the ColabFold databases of proteins using MMseqs2 to build diverse multiple sequence alignments in A3M format.</li> <li>A script colabfold_batch designed to predict protein structures.</li> <li>A set of protein databases:<ul> <li>UniRef30: database containing 30% sequence identity clustered proteins based on UniRef100 non-redundant protein sequence database. </li> <li>Environmental database (also called ColabFoldDB): combination of the Big Fantastic Database (BFD) and the MGnify database with redundancy reduced in addition to metagenomic protein catalogs containing eukaryotic proteins, phage catalogs and an updated version of MetaClust.</li> <li>Templates database (PDB70): database containing 70% sequence identity clustered proteins from the Protein Data Bank (PDB) database.</li> </ul> </li> </ol>"},{"location":"tools/ColabFold.html#getting-started","title":"Getting started","text":""},{"location":"tools/ColabFold.html#search-against-the-colabfold-databases","title":"Search against the ColabFold databases","text":"<p>Finding homologous proteins using MMSeqs2 can be done by running a batch script using the SBATCH command: <code>sbatch &lt;path to batch script file&gt;</code></p> <p>The code below is an example of a batch script to run MMSeqs2. The top of the script contains the instruction to use bash to execute the commands (<code>#!/bin/bash</code>) and the SBATCH parameters (<code>#SBATCH &lt;parameter&gt;</code>) followed by the different modules required to run MMSeqs2 through ColabFold.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=cpu,cpu-preempt,cpu-long\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem=200G\n#SBATCH -t 05:00:00\n#SBATCH -o slurm-%j.out\n#SBATCH -e slurm-%j.err\n\nmodule load uri/main\nmodule load MMseqs2/14-7e284-gompi-2021b\nmodule load cudnn/cuda11-8.4.1.50\nmodule load cuda/11.4.0\nmodule load miniconda/22.11.1-1\nsource ~/.bashrc\nconda activate colabfold\n</code></pre> <p>The command <code>colabfold_search</code> shown below is added after activating the conda environment <code>colabfold</code>. In this case, protein sequences contained in a fasta file are aligned against the UniRef30 (<code>--db1 uniref30_2202/uniref30_2202_db</code>) and environmental (<code>--db3 colabfold_envdb_202108/colabfold_envdb_202108_db</code>) databases. UniRef30 is the default database used to search proteins. In order to use the environmental database, the parameter <code>--use-env</code>has to be set to 1 in addition to providing the path (<code>--db3 colabfold_envdb_202108/colabfold_envdb_202108_db</code>).</p> <pre><code>colabfold_search &lt;path to fasta file&gt; /datasets/bio/colabfold &lt;path to output directory&gt; --db1 uniref30_2202/uniref30_2202_db --db3 colabfold_envdb_202108/colabfold_envdb_202108_db --use-env 1 --use-templates 0 --threads $SLURM_CPUS_ON_NODE\n</code></pre> <p>To use the PDB70 templates database, the parameter <code>--use-templates</code> should be set to 1 and the path to the database should be provided with <code>--db2 pdb</code>.</p>"},{"location":"tools/ColabFold.html#notes_1","title":"Notes:","text":"<ul> <li><code>&lt;path to fasta file&gt;</code> is the full path to a fasta file containing protein sequence(s) of interest.</li> <li><code>&lt;path to output directory&gt;</code> is the full path to an existing directory used to store the multiple sequence alignments (MSAs).</li> <li>Note that it is recommended to request at least 200G using <code>#SBATCH --mem=200G</code> in order to load the protein databases.</li> <li>Running colabfold_search with 1,762 proteins, the UniRef30 and environmental databases and the highest mmseqs sensitivity (s = 8) on a gpu A100 node with 64 threads takes approximately 3h.</li> </ul>"},{"location":"tools/ColabFold.html#make-predictions-with-colabfold-using-a-batch-script","title":"Make predictions with ColabFold using a batch script","text":"<p>A batch script can be used to make predictions with ColabFold. It should be noted that predictions on proteins longer than 2000 amino acids should be run on a GPU node with at least 16GB VRAM and that the whole process can be expedited on a large set of input protein sequences by submitting the batch script as an array job.</p> <p>The code below provides an example on how to make predictions using <code>colabfold_batch</code> in a batch script. The SBATCH options specify resources including access to a GPU with at least 16GB of VRAM (<code>--constraint=vram16</code>) necessary to run colabfold on a protein of 2894 amino acids.</p> <p>The parameter <code>--stop-at-score</code> is used to stop generating models until the predicted confidence metric (pLDDT or predicted local distance difference test) is reached.</p> <pre><code>#!/bin/bash\n#SBATCH --partition=gpu,gpu-preempt,gpu-long\n#SBATCH --constraint=vram16\n#SBATCH --gpus-per-node=1\n#SBATCH --cpus-per-gpu=8\n#SBATCH --mem-per-gpu=40G\n#SBATCH -t 05:00:00\n#SBATCH -o slurm-%j.out\n#SBATCH -e slurm-%j.err\n\nmodule load cudnn/cuda11-8.4.1.50\nmodule load cuda/11.4.0\nmodule load miniconda/22.11.1-1\nsource ~/.bashrc\n\nconda activate colabfold\n\ncolabfold_batch &lt;path to directory containing MSAs&gt; &lt;path to output directory&gt; --stop-at-score 85\n</code></pre> <p>The <code>colabfold_batch</code> command above will create the following files in the provided output directory for each input protein sequence:</p> <ul> <li> <p><code>{*}_PAE.png</code> \u2192 2D plot of the Predicted Aligned Error (PAE) for each of the 5 trained models.</p> </li> <li> <p><code>{*}_coverage.png</code> \u2192 plot of the coverage of protein sequences to the query protein.</p> </li> <li> <p><code>{*}_plddt.png</code> \u2192 plot of the pLDDT (predicted local distance difference test) scores for each residue and the 5 trained models.</p> </li> <li> <p><code>{*}_predicted_aligned_error_v1.json</code> \u2192 raw data with PAE for all residue pairs for each of the 5 trained models.</p> </li> </ul> <p>The next 2 files are generated for the 5 trained models:</p> <ul> <li><code>{*}_unrelaxed_rank_1_model_1.pdb</code> \u2192 PDB format text file containing the predicted structure obtained from model 1.</li> <li><code>{*}_unrelaxed_rank_1_model_1_scores.json</code> \u2192 raw data with the pLDDT scores for each residue of the protein structure obtained from model 1.</li> </ul>"},{"location":"tools/ColabFold.html#notes_2","title":"Notes:","text":"<ul> <li><code>&lt;path to directory containing MSAs&gt;</code> is the same as <code>&lt;path to the output directory&gt;</code> used with the <code>colabfold_search</code> command.</li> <li><code>&lt;path to output directory&gt;</code> is the full path to an existing directory used to store the results.</li> <li>When dealing with a large number of sequences, we recommended sorting proteins into batches based on their size and submitting a job to a GPU node with smaller VRAM for batches with shorter proteins.</li> <li>Note that one of colabfold default settings is to not overwrite existing results. Therefore, the batch script of a job that ended before colabfold finished can be resubmitted and colabfold will pursue making predictions for the remaining protein sequences.</li> </ul>"},{"location":"tools/ColabFold.html#full-list-of-parameters-for-colabfold_search-and-colabfold_batch","title":"Full list of parameters for colabfold_search and colabfold_batch","text":"<pre><code>colabfold_search [-h] [-s S] [--db1 DB1] [--db2 DB2] [--db3 DB3]\n                        [--use-env {0,1}] [--use-templates {0,1}]\n                        [--filter {0,1}] [--mmseqs MMSEQS]\n                        [--expand-eval EXPAND_EVAL] [--align-eval ALIGN_EVAL]\n                        [--diff DIFF] [--qsc QSC] [--max-accept MAX_ACCEPT]\n                        [--db-load-mode DB_LOAD_MODE] [--threads THREADS]\n                        query dbbase base\n\n  query                 fasta files with the queries.\n  dbbase                The path to the database and indices you downloaded\n                        and created with setup_databases.sh\n  base                  Directory for the results (and intermediate files)\n  -s S                  mmseqs sensitivity (1-8). Lowering this will result in a\n                        much faster search but possibly sparser msas \u2192 default = 8\n  --db1 DB1             path to a UniRef database on Unity\n  --db2 DB2             path to the Templates database on Unity\n  --db3 DB3             path to the Environmental database on Unity\n  --use-env {0,1}\n  --use-templates {0,1}\n  --filter {0,1}\n  --mmseqs MMSEQS       Location of the mmseqs binary\n  --expand-eval EXPAND_EVAL\n  --align-eval ALIGN_EVAL\n  --diff DIFF\n  --qsc QSC\n  --max-accept MAX_ACCEPT\n  --db-load-mode DB_LOAD_MODE \u2192 default = 0 (batch searches)\n  --threads THREADS\n</code></pre> <pre><code>colabfold_batch [-h]   [--stop-at-score STOP_AT_SCORE]\n                       [--stop-at-score-below STOP_AT_SCORE_BELOW]\n                       [--num-recycle NUM_RECYCLE]\n                       [--num-ensemble NUM_ENSEMBLE]\n                       [--random-seed RANDOM_SEED] [--num-models {1,2,3,4,5}]\n                       [--recompile-padding RECOMPILE_PADDING]\n                       [--model-order MODEL_ORDER] [--host-url HOST_URL]\n                       [--data DATA]\n                       [--msa-mode {'mmseqs2_uniref_env','mmseqs2_uniref','single_sequence'}]\n                       [--model-type {auto,AlphaFold2-ptm,AlphaFold2-multimer-v1,AlphaFold2-multimer-v2}]\n                       [--amber] [--templates]\n                       [--custom-template-path CUSTOM_TEMPLATE_PATH] [--env]\n                       [--cpu] [--rank {auto,plddt,ptmscore,multimer}]\n                       [--pair-mode {unpaired,paired,unpaired+paired}]\n                       [--recompile-all-models]\n                       [--sort-queries-by {none,length,random}]\n                       [--save-single-representations]\n                       [--save-pair-representations] [--training]\n                       [--max-msa           {512:5120,512:1024,256:512,128:256,64:128,32:64,16:32}]\n                       [--zip] [--use-gpu-relax]\n                       [--overwrite-existing-results]\n                       input results\n\n--stop-at-score STOP_AT_SCORE\n                        Compute models until plddt (single chain) or ptmscore\n                        (complex) &gt; threshold is reached. This can make\n                        colabfold much faster by only running the first model\n                        for easy queries.\n--stop-at-score-below STOP_AT_SCORE_BELOW \u2192 default = 0\n                        Stop to compute structures if plddt (single chain) or\n                        ptmscore (complex) &lt; threshold. This can make\n                        colabfold much faster by skipping sequences that do\n                        not generate good scores.\n--num-recycle NUM_RECYCLE &lt;strong&gt;\u2192 default = 3&lt;/strong&gt;\n                        Number of prediction cycles.Increasing recycles can\n                        improve the quality but slows down the prediction.\n--num-ensemble NUM_ENSEMBLE &lt;strong&gt;\u2192 default = 1&lt;/strong&gt;\n                        Number of ensembles.The trunk of the network is run\n                        multiple times with different random choices for the\n                        MSA cluster centers.\n--random-seed RANDOM_SEED &lt;strong&gt;\u2192 default = 0&lt;/strong&gt;\n                        Changing the seed for the random number generator can\n                        result in different structure predictions.\n--num-models {1,2,3,4,5} &lt;strong&gt;\u2192 default = 5&lt;/strong&gt;\n--recompile-padding RECOMPILE_PADDING &lt;strong&gt;\u2192 default = 1.1&lt;/strong&gt;\n                        Whenever the input length changes, the model needs to\n                        be recompiled, which is slow. We pad sequences by this\n                        factor, so we can e.g. compute sequence from length\n                        100 to 110 without recompiling. The prediction will\n                        become marginally slower for the longer input, but\n                        overall performance increases due to not recompiling.\n                        Set to 1 to disable.\n--model-order MODEL_ORDER\n--host-url HOST_URL &lt;strong&gt;\u2192 default = https://api.colabfold.com&lt;/strong&gt;\n--data DATA\n--msa-mode {MMseqs2 (UniRef+Environmental),MMseqs2 (UniRef only),single_sequence}\n                        Using an a3m file as input overwrites this option\n--model-type {auto,AlphaFold2-ptm,AlphaFold2-multimer-v1,AlphaFold2-multimer-v2}\n                        predict structure/complex using the following\n                        model.Auto will pick \"AlphaFold2\" (ptm) for structure\n                        predictions and \"AlphaFold2-multimer-v2\" for\n                        Complexes. &lt;strong&gt;\u2192 default = AlphaFold2-ptm&lt;/strong&gt;\n--amber               Use amber for structure refinement\n--templates           Use templates from pdb\n--custom-template-path CUSTOM_TEMPLATE_PATH\n                        Directory with pdb files to be used as input&lt;/code&gt;\n--env\n--cpu                 Allow running on the cpu, which is very slow\n--rank {auto,plddt,ptmscore,multimer} \u2192 default = plddt\n                        rank models by auto, plddt or ptmscore\n--pair-mode {unpaired,paired,unpaired+paired} \u2192 default =  unpaired+paired\n                        rank models by auto, unpaired, paired, unpaired+paired\n--recompile-all-models \u2192 default = false\n                        recompile all models instead of just model 1 and 3\n--sort-queries-by {none,length,random}\n                        sort queries by: none, length, random\n--save-single-representations\n                        saves the single representation embeddings of all\n                        models\n--save-pair-representations\n                        saves the pair representation embeddings of all models\n--training            turn on training mode of the model to activate drop\n                        Outs \u2192 default = false\n--max-msa {512:5120,512:1024,256:512,128:256,64:128,32:64,16:32} \u2192 default = null\n                        defines: `max_msa_clusters:max_extra_msa` number of\n                        sequences to use\n--zip                 zip all results into one &lt;jobname&gt;.result.zip and\n                        delete the original files\n--use-gpu-relax       run amber on GPU instead of CPU\n--overwrite-existing-results \u2192 default = false\n</code></pre>"},{"location":"tools/ColabFold.html#references","title":"References","text":"<ol> <li>Mirdita, M., Sch\u00fctze, K., Moriwaki, Y. et al. ColabFold: making protein folding accessible to all. Nat Methods 19, 679\u2013682 (2022). https://doi.org/10.1038/s41592-022-01488-1</li> <li>Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583\u2013589 (2021). https://doi.org/10.1038/s41586-021-03819-2</li> <li>https://github.com/sokrypton/ColabFold#running-locally</li> </ol>"},{"location":"tools/GPU.html","title":"Unity GPU Documentation","text":"<p>Graphics Processing Units (GPUs) provide a powerful tool to run code in parallel at a larger scale than traditional CPU parallel workload. This comes at the tradeoff with slower communication times. It is important to note that using one or more GPUs does not guarantee that code will run faster, however many popular software packages have been modified to incorporate GPUs for better performance.</p>"},{"location":"tools/GPU.html#available-gpu-resources","title":"Available GPU Resources","text":"Device Arch Caps VRAM Constraint(s) NVIDIA GeForce GTX TITAN X Maxwell sm_52 vram8 vram11 vram12 titanx Tesla M40 24GB Maxwell sm_52 vram8 vram11 vram12 vram16 vram23 m40 NVIDIA GeForce GTX 1080 Ti Pascal sm_52 sm_61 vram8 vram11 1080ti Tesla V100-PCIE-16GB Volta sm_52 sm_61 sm_70 vram8 vram11 vram12 vram16 v100 Tesla V100-SXM2-16GB Volta sm_52 sm_61 sm_70 vram8 vram11 vram12 vram16 v100 Tesla V100-SXM2-32GB Volta sm_52 sm_61 sm_70 vram8 vram11 vram12 vram16 vram23 vram32 v100 NVIDIA GeForce RTX 2080 Turing sm_52 sm_61 sm_70 sm_75 vram8 2080 NVIDIA GeForce RTX 2080 Ti Turing sm_52 sm_61 sm_70 sm_75 vram8 vram11 2080ti Quadro RTX 8000 Turing sm_52 sm_61 sm_70 sm_75 vram8 vram11 vram12 vram16 vram23 vram32 vram40 vram48 rtx8000 NVIDIA A100-PCIE-40GB Ampere sm_52 sm_61 sm_70 sm_75 sm_80 vram8 vram11 vram12 vram16 vram23 vram32 vram40 a100, a100-40g NVIDIA A100-SXM4-80GB Ampere sm_52 sm_61 sm_70 sm_75 sm_80 vram8 vram11 vram12 vram16 vram23 vram32 vram40 vram48 vram80 a100, a100-80g"},{"location":"tools/GPU.html#requesting-gpu-resources","title":"Requesting GPU Resources","text":"<p>Requesting GPU access on Unity can be done via Slurm either for an interactive session or using a batch script. Below are a minimal example of both interactive and batch jobs.</p> <p>Note</p> <p>Not all software is able to use GPUs, and some software will require special options, dependencies, or alternate versions to be able to run with GPUs. Please ensure your software supports GPU use before requesting these resources.</p> <p>Interactive</p> <pre><code>srun -p gpu-preempt -t 02:00:00 --gpus=1 --pty /bin/bash\n</code></pre> <p>Batch Script</p> <pre><code>#!/bin/bash\n\n#SBATCH -p gpu-preempt # Submit job to to gpu-preempt partition\n#SBATCH -t 02:00:00    # Set max job time for 2 hours\n#SBATCH --gpus=1       # Request access to 1 GPU\n$SBATCH --constraint=2080ti # Request access to a 2080ti GPU\n\n./myscript.sh\n</code></pre> <p>Specific GPUs can also be selected by using the <code>--constraint</code> flags with Slurm, or by adding the gpu type to <code>--gpus</code>. The available constraints are listed below.</p> <p>Note</p> <p>Using <code>--constraint</code> allows you to select multiple possible GPUs that fulfil the requirements. You can either use <code>--constraint=[2080|2080ti]</code> or <code>--constraint=sm_70&amp;vram12</code>. It is better to use the first form if you are using GPUs across more than one node to ensure the same model is used across all entire job.</p> <ul> <li>2080ti</li> <li>1080ti</li> <li>2080</li> <li>titanx</li> <li>m40</li> <li>rtx8000</li> <li>v100</li> <li>a100</li> </ul> <p>Batch Script with Specific GPU</p> <pre><code>#!/bin/bash\n\n#SBATCH -p gpu-preempt # Submit job to to gpu-preempt partition\n#SBATCH -t 02:00:00    # Set max job time for 2 hours\n#SBATCH --gpus=2080ti:1       # Request access to 1 2080tiGPU\n\n./myscript.sh\n</code></pre> <p>Batch Script with Constraint</p> <pre><code>#!/bin/bash\n\n#SBATCH -p gpu-preempt # Submit job to to gpu-preempt partition\n#SBATCH -t 02:00:00    # Set max job time for 2 hours\n#SBATCH --gpus=1       # Request access to 1 2080tiGPU\n#SBATCH --constraint=2080ti\n\n./myscript.sh\n</code></pre> <p>Batch Script with Constraint specifying multiple options</p> <pre><code>#!/bin/bash\n\n#SBATCH -p gpu-preempt # Submit job to to gpu-preempt partition\n#SBATCH -t 02:00:00    # Set max job time for 2 hours\n#SBATCH --gpus=1       # Request access to 1 2080tiGPU\n#SBATCH --constraint=2080ti|1080ti|2080\n\n./myscript.sh\n</code></pre>"},{"location":"tools/GPU.html#gpu-enabled-software","title":"GPU-Enabled Software","text":"<p>CUDA: NVIDIA's parallel computing platform. A version of this will typically be required to be loaded for most GPU jobs, as this allows access to this NVIDIA compiler suite (nvcc, nvfortran) as well as the NVIDIA GPU profiling tool (nsys).</p> <p>cuDNN: Cuda Deep Neural Network library, often used to accelerate deep learning frameworks in Keras, PyTorch, TensorFlow, and others.</p> <p>OpenMPI: The OpenMPI compilers for MPI compiled against the cuda compilers. This is necessary to use if software that uses both MPI and GPU acceleration.</p> <p>Note: be sure to check which version(s) of cuda are compatible with the software that is being used.</p> Software Name Available Verions cuda 11 11.8.0, 11.5.0, 11.4.0, 11.3.1, 11.0.3, 11.0.1 cuda 10 10.2.89, 10.1.243, 10.0.130 cuda legacy versions (&lt;10.0) 9.2, 9.2.88, 9.0, 8.0.61, 8.0, 7.5.18, 7.0, 6.5.14, 6.0 cudnn cuda11-8.4.1.50, cuda10-7.5.0.56, 8.2.4.15-11.4 openmpi 4.1.3+cuda11.6.2-mpirun, 4.1.3+cuda11.6.2 <p>In addition to these, many programming languages are able to use one or more GPUs.</p> <ul> <li>Python</li> <li>Matlab</li> <li>Julia</li> <li>C++ (using Cuda or OpenACC)</li> <li>Fortran (using Cuda or OpenACC)</li> <li>C (using Cuda or OpenACC)</li> </ul>"},{"location":"tools/GPU.html#setting-up-a-tensorflow-gpu-environment","title":"Setting up a TensorFlow GPU Environment","text":"<p>Some software, especially with python, requires setting up the environment in a specific way.</p> <p>For python programs that can use GPU, such as TensorFlow, this is best done using a conda environment.</p> <p>The steps to set up a conda environment for TensorFlow is shown below:</p> <ol> <li>request an interactive session with a GPU node</li> </ol> <pre><code>srun -t 01:00:00 -p gpu-preempt --gpus=1 --mem=16G --pty /bin/bash\n</code></pre> <ol> <li>load modules</li> </ol> <pre><code>module load miniconda/22.11.1-1\nmodule load cuda/11.4.0\nmodule load cudnn/cuda11-8.4.1.50\n</code></pre> <ol> <li>create the environment</li> </ol> <pre><code>conda create --name TensorFlow-env python=3.9\n</code></pre> <p>Note: TensorFlow 2 requires a python version of at least 3.9</p> <pre><code>conda activate TensorFlow-env\npip install TensorFlow\npip install tensorrt\nconda install ipykernel\n</code></pre> <p>Note: if you do not request enough memory, TensorRT will fail to install</p> <ol> <li>Add environment to Jupyter</li> </ol> <pre><code>python -m ipykernel install --user --name TensorFlow-env --display-name=\"TensorFlow-Env\"\n</code></pre> <p>After completing these steps, a new kernel with the name \"TensorFlow-Env\" will be shown with new Open OnDemand sessions</p>"},{"location":"tools/GPU.html#troubleshooting-with-gpus","title":"Troubleshooting with GPUs","text":"<p>To view ongoing GPU processes, the <code>nvidia-smi pmon</code> command can be used.</p> <p>If you are getting error messages, please be sure to add the following command to your scripts in order to know which GPU is being used.</p> <pre><code>nvidia-smi -L\n</code></pre> <p>If there is a CUDA_ERROR_OUT_OF_MEMORY, a GPU with more available VRAM may be necessary, or the code being run should be modified to reduce the memory usage.</p>"},{"location":"transfers/transfers.html","title":"Transferring data from the Shared Cluster (GHPCC) to Unity:","text":""},{"location":"transfers/transfers.html#this-guide-will-allow-you-to-transfer-data-from-the-shared-cluster-ghpcc-to-unity-via-globus","title":"This guide will allow you to transfer data from the Shared Cluster (GHPCC) to Unity via Globus.","text":""},{"location":"transfers/transfers.html#step-1-log-in-to-globus","title":"Step 1: Log in to Globus","text":"<ul> <li>Visit this site: https://app.globus.org/file-manager</li> <li>Use your university credentials to log in</li> </ul>"},{"location":"transfers/transfers.html#step-2-connect-to-the-shared-cluster-and-unity","title":"Step 2: Connect to the Shared Cluster and Unity","text":"<ul> <li>Select the File Manager tab from lefthand pane</li> <li>In the two \"Collection\" boxes, select the Shared Cluster and Unity via their Globus identities<ul> <li>The Shared Cluster is ghpcc#ghpcc07</li> <li>Unity is Unity<ul> <li>more technical identifier: acda5457-9c06-4564-8375-260ba428f22a</li> </ul> </li> <li>If you start typing either in the \"Collection\" text box, the drop-down will update and allow you to select the appropriate \"Collection\" (aka cluster)</li> </ul> </li> <li>Provide your login credentials for each cluster<ul> <li>Once you're finished logging in to each connection, the screen should look something like this: </li> </ul> </li> </ul>"},{"location":"transfers/transfers.html#step-3-transfer-files","title":"Step 3: Transfer files","text":"<ul> <li>Below, the Connection and Path text boxes, you should see the contents of each directory.</li> <li>Navigate to the location of the file/directory you'd like to transfer from and to.</li> <li>Select the file/directory you'd like to transfer</li> <li>Click on the \"Transfer or Sync to...\" button. Shown in the middle here: </li> <li>Voila! The file/directory will be queued up and transferred shortly!</li> </ul>"},{"location":"transfers/transfers.html#step-4-confirm-that-transfer-was-successful","title":"Step 4: Confirm that transfer was successful","text":"<ul> <li>You'll receive an email once the transfer occurs.</li> <li>Open the Destination directory and confirm that the file was transferred as expected.</li> </ul>"},{"location":"transfers/transfers.html#this-guide-will-allow-you-to-send-data-from-the-shared-cluster-to-the-unity-server-using-scp-for-the-purposes-of-transitioning","title":"This guide will allow you to send data from the Shared Cluster to the Unity server using <code>scp</code> for the purposes of transitioning.","text":""},{"location":"transfers/transfers.html#notes","title":"Notes:","text":"<ul> <li>You will need to have set up an account on the Unity server.</li> <li>You will need your password for the Shared Cluster server (unless you are using private/public keys for logging on to this too).</li> </ul>"},{"location":"transfers/transfers.html#step-1","title":"Step 1.","text":"<ul> <li>Log in to the Shared Cluster (GHPCC).</li> <li>While logged into the Shared Cluster, from your home directory, run <code>ssh-keygen -t rsa</code><ul> <li>This will generate a private/public key pair which is unique to each user.</li> <li>The generator will ask you to save the keys as a specific name, and you have the option to associate a password to the keys (recommended but not required).</li> <li>Two files will be created: NAME and NAME.pub</li> <li>NAME.pub is the public key that you will need for Step 2.</li> </ul> </li> </ul>"},{"location":"transfers/transfers.html#step-2","title":"Step 2.","text":"<ul> <li>Log in to the Unity cluster portal: https://unity.rc.umass.edu/</li> <li>Navigate to \"Account Settings\"</li> <li>You will see the SSH keys that are currently linked to your account</li> <li>Click the \"+\" button to add the Shared Cluster key to your account.</li> <li>Copy the entire contents of the public key (NAME.pub) from the Shared Cluster generated in Step 1 into the prompt.</li> <li>Click \"add key\"</li> <li>Click \"Set Login Shell\"<ul> <li>This will now link the two servers, allowing them to communicate </li> </ul> </li> </ul>"},{"location":"transfers/transfers.html#step-3","title":"Step 3.","text":"<ul> <li>Log into the Unity cluster</li> <li>Create a file to test the connection between servers (e.g. <code>touch test.txt</code>)</li> <li>Attempt to transfer the file from Unity to the Shared Cluster:<ul> <li><code>scp test.txt username@ghpcc06.umassrc.org:/home/username/</code></li> </ul> </li> <li>You will be prompted for your password to the Shared Cluster - enter it here.</li> <li>If configured properly, this file should transfer immediately to the specified destination path (in this case <code>/home/username/</code>)</li> </ul>"},{"location":"transfers/transfers.html#step-4","title":"Step 4.","text":"<ul> <li>Assuming that Step 3 worked and the file transferred across from Unity to the Shared Cluster, you can use this to pull data across: <code>scp username@ghpcc06.umassrc.org:/where/files/are/you/want/* ./destination/</code></li> <li>Each time you run <code>scp</code> in this way, you will be prompted for your password.</li> <li>Note that currently this only works when logged in and running commands from Unity, not the Shared Cluster.</li> </ul> <p>Globus documentation provided by the Molecular Ecology and Conservation Lab, Dept. Environmental Conservation.</p>"},{"location":"updates/index.html","title":"Cluster Updates","text":""},{"location":"updates/index.html#the-module-hierarchy-change-coming-soon","title":"The Module Hierarchy Change (Coming soon!)","text":"<p>As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward.</p> <p>The <code>module</code> command refers to Environment Modules. We use Lmod, which is the Lua flavor of Environment Modules. The <code>$MODULEPATH</code> environment variable is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default.</p> <p>This means not all modules can be found with <code>module avail</code> by default.</p>"},{"location":"updates/index.html#what-does-this-mean-for-my-workflow","title":"What does this mean for my workflow?","text":"<p>Without extra steps you may encounter the following error:</p> <pre><code>No module(s) or extension(s) found!\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>In this case you can follow the documentation on how to use the hierarchy.</p> <p>In short, you use the <code>unity-module-find</code> command, and from that output you should be able to tell which other modules need to be loaded first.</p> <p>In many cases, it can be as simple as changing <code>module load gromacs</code> into <code>module load openmpi gromacs</code>.</p>"},{"location":"updates/index.html#does-this-affect-me","title":"Does this affect me?","text":"<p>The following modules are no longer in the default modulepath:</p> <ul> <li>modules built with the intel classic compilers<ul> <li>zlib/1.2.12-intel@2021.4.0</li> <li>pigz</li> </ul> </li> <li>modules that use mpi<ul> <li>cgns</li> <li>fftw/3.3.8+openmpi4.1.3</li> <li>fftw/3.3.10+intel-oneapi-mpi2021.6.0</li> <li>grace</li> <li>gromacs</li> <li>med</li> <li>mmg</li> <li>netcdf</li> <li>openfoam</li> <li>valgrind</li> </ul> </li> <li>modules built for specific CPU micro-architectures<ul> <li>cascadelake</li> <li>haswell</li> <li>icelake</li> <li>skylake_avx512</li> <li>zen</li> <li>zen2</li> </ul> </li> </ul>"},{"location":"updates/index.html#why","title":"Why?","text":""},{"location":"updates/index.html#module-name-length","title":"Module name length","text":"<p>Historically we have tried to make each module name unique. For example:</p> <pre><code>fftw/3.3.8+openmpi4.1.3\nfftw/3.3.8+intel-oneapi-mpi2021.6.0\n</code></pre> <p>This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns.</p> <p>Gromacs, for example: <code>gromacs/2021.3+openmpi4.1.3-intel@2021.4</code> would be shortened to <code>gromacs/2021.3</code>.</p>"},{"location":"updates/index.html#module-name-conflicts","title":"Module name conflicts","text":"<p>If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a <code>(D)</code> flag. The only way to get around this is to manually manipulate <code>$MODULEPATH</code> to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority.</p>"},{"location":"updates/index.html#module-conflicts","title":"Module Conflicts","text":"<p>Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters.</p> <p>With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes.</p> <p>If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same.</p> <p>From the Lmod documentation:</p> <p>it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated.</p> <p>A module hierarchy is a robust and elegant solution to avoid module conflicts.</p>"},{"location":"updates/index.html#what-is-going-on-under-the-hood","title":"What is going on under the hood?","text":"<p>Our current module tree has two main directories with many modules inside.</p> <pre><code>/modules/modulefiles\n/modules/spack_modulefiles\n</code></pre> <p>We are simply splitting it into multiple directories.</p> <pre><code>/modules/modulefiles\n/modules/spack_modulefiles\n\u251c\u2500\u2500 linux-ubuntu20.04-x86_64\n|   \u251c\u2500\u2500 Core\n|   \u251c\u2500\u2500 intel/2021.4.0\n|   \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0\n|   \u2502   \u2514\u2500\u2500 openblas/0.3.18\n|   \u251c\u2500\u2500 openblas/0.3.18\n|   \u251c\u2500\u2500 openmpi/4.1.3\n|   |   \u2514\u2500\u2500 intel-mkl/2020.4.304\n|   \u251c\u2500\u2500 openmpi/4.1.4\n|   |   \u2514\u2500\u2500 intel-mkl/2020.4.304\n|   \u2514\u2500\u2500 atlas/3.10.3\n\u251c\u2500\u2500 linux-ubuntu20.04-aarch64\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 linux-ubuntu20.04-ppc64le\n    \u2514\u2500\u2500 ...\n\n</code></pre> <p>The Unity module set is split in two. <code>/modules/modulefiles</code> contains our homebrew modules, those compiled by hand by the admins. <code>/modules/spack_modulefiles</code> contains modules created by Spack. <code>/modules/modulefiles</code> is not changing.</p> <p>At the start of each new login shell, <code>$MODULEPATH</code> contains only <code>/modules/modulefiles/</code> and <code>/modules/spack_modulefiles/linux-ubuntu20.04-&lt;architecture&gt;/Core</code>. The \"Core\" directory contains the modules compiled with Ubuntu's default GNU compiler suite, and without any special providers. Modules made with another compiler, or with special providers, are separated from Core and given their own directory. When you <code>module load</code> a compiler or special provider, its directory is added to <code>$MODULEPATH</code>, and any modules in that directory are now available to use.</p>"},{"location":"updates/index.html#also-included-in-this-change","title":"Also included in this change","text":"<ul> <li>Lmod will no longer allow you to load modules without a version name.</li> <li>Every new Slurm job will <code>module purge</code>. You will have to make sure that your jobs load modules all by themselves.<ul> <li>This is to ensure that incompatible modules are not used.</li> </ul> </li> <li>New commands are available in your shell:<ul> <li><code>unity-module-find</code></li> <li><code>unity-module-hierarchy</code></li> <li><code>unity-module-hierarchy-help</code></li> <li><code>unity-slurm-list-constraints</code></li> <li><code>unity-slurm-find-nodes</code></li> </ul> </li> <li>We are switching Spack TCL modules to Lua.<ul> <li>This shouldn't affect the user experience.</li> </ul> </li> </ul>"},{"location":"updates/index.html#learn-more","title":"Learn more","text":"<p>Using the Module Hierarchy</p> <p>https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy</p> <p>https://lmod.readthedocs.io/en/latest/080_hierarchy.html</p>"},{"location":"updates/hierarchy-change.html","title":"The Module Hierarchy Change (Coming soon!)","text":"<p>As a Unity user, you have access to many modules built with various software stacks. As Unity grows and more modules are installed with more stacks, it can become difficult to effectively manage them all. The hierarchy change is to combat module bloat, simplify module names, and (most importantly) prevent conflicts. For many users, nothing will change, but some will have to take a few extra steps when loading modules going forward.</p> <p>The <code>module</code> command refers to Environment Modules. We use Lmod, which is the Lua flavor of Environment Modules. The <code>$MODULEPATH</code> environment variable is a list of directories in which Lmod searches for modules. With a module hierarchy, not all directories are added to the modulepath by default.</p> <p>This means not all modules can be found with <code>module avail</code> by default.</p>"},{"location":"updates/hierarchy-change.html#what-does-this-mean-for-my-workflow","title":"What does this mean for my workflow?","text":"<p>Without extra steps you may encounter the following error:</p> <pre><code>No module(s) or extension(s) found!\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>In this case you can follow the documentation on how to use the hierarchy.</p> <p>In short, you use the <code>unity-module-find</code> command, and from that output you should be able to tell which other modules need to be loaded first.</p> <p>In many cases, it can be as simple as changing <code>module load gromacs</code> into <code>module load openmpi gromacs</code>.</p>"},{"location":"updates/hierarchy-change.html#does-this-affect-me","title":"Does this affect me?","text":"<p>The following modules are no longer in the default modulepath:</p> <ul> <li>modules built with the intel classic compilers<ul> <li>zlib/1.2.12-intel@2021.4.0</li> <li>pigz</li> </ul> </li> <li>modules that use mpi<ul> <li>cgns</li> <li>fftw/3.3.8+openmpi4.1.3</li> <li>fftw/3.3.10+intel-oneapi-mpi2021.6.0</li> <li>grace</li> <li>gromacs</li> <li>med</li> <li>mmg</li> <li>netcdf</li> <li>openfoam</li> <li>valgrind</li> </ul> </li> <li>modules built for specific CPU micro-architectures<ul> <li>cascadelake</li> <li>haswell</li> <li>icelake</li> <li>skylake_avx512</li> <li>zen</li> <li>zen2</li> </ul> </li> </ul>"},{"location":"updates/hierarchy-change.html#why","title":"Why?","text":""},{"location":"updates/hierarchy-change.html#module-name-length","title":"Module name length","text":"<p>Historically we have tried to make each module name unique. For example:</p> <pre><code>fftw/3.3.8+openmpi4.1.3\nfftw/3.3.8+intel-oneapi-mpi2021.6.0\n</code></pre> <p>This has worked well enough, but long module names make it harder for Lmod to display many modules on one screen, because horizontal whitespace can be used for more columns.</p> <p>Gromacs, for example: <code>gromacs/2021.3+openmpi4.1.3-intel@2021.4</code> would be shortened to <code>gromacs/2021.3</code>.</p>"},{"location":"updates/hierarchy-change.html#module-name-conflicts","title":"Module name conflicts","text":"<p>If module names are not unique, it can be difficult to choose which module to load. When two modules from different directories have the same name, Lmod will decide which is default and mark it with a <code>(D)</code> flag. The only way to get around this is to manually manipulate <code>$MODULEPATH</code> to exclude certain directories until the desired module is marked as default. With the hierarchy, it's implicit that if you add special modules to your modulepath, you want those special modules to take priority.</p>"},{"location":"updates/hierarchy-change.html#module-conflicts","title":"Module Conflicts","text":"<p>Some software (particularly that which uses mpi) needs to be built in a similar manner to its dependencies. NetCDF, for example, will not work unless HDF5 and MPI are built with the same compiler and with certain parameters.</p> <p>With modules specific to CPU micro-architectures, it's possible to compile software on one node that is unable to run on other nodes. For example, linking off of Skylake modules may lead to problems when running on older Haswell nodes.</p> <p>If you try to compile using a set of incompatible modules, bad things can happen. At best, Cmake/Autotools will tell you to use something else; At worst, you can struggle with nebulous compiler errors for hours on end. At runtime the result is largely the same.</p> <p>From the Lmod documentation:</p> <p>it is quite easy to load an incompatible set of modules. Now, it is possible that the system administrators at your site might have set up conflicts to avoid loading mismatched modules. However, using conflicts can be fragile. What happens if a site adds a new compiler such as clang or pgi or a new mpi stack? All those module file conflict statements will have to be updated.</p> <p>A module hierarchy is a robust and elegant solution to avoid module conflicts.</p>"},{"location":"updates/hierarchy-change.html#what-is-going-on-under-the-hood","title":"What is going on under the hood?","text":"<p>Our current module tree has two main directories with many modules inside.</p> <pre><code>/modules/modulefiles\n/modules/spack_modulefiles\n</code></pre> <p>We are simply splitting it into multiple directories.</p> <pre><code>/modules/modulefiles\n/modules/spack_modulefiles\n\u251c\u2500\u2500 linux-ubuntu20.04-x86_64\n|   \u251c\u2500\u2500 Core\n|   \u251c\u2500\u2500 intel/2021.4.0\n|   \u251c\u2500\u2500 intel-oneapi-mpi/2021.6.0\n|   \u2502   \u2514\u2500\u2500 openblas/0.3.18\n|   \u251c\u2500\u2500 openblas/0.3.18\n|   \u251c\u2500\u2500 openmpi/4.1.3\n|   |   \u2514\u2500\u2500 intel-mkl/2020.4.304\n|   \u251c\u2500\u2500 openmpi/4.1.4\n|   |   \u2514\u2500\u2500 intel-mkl/2020.4.304\n|   \u2514\u2500\u2500 atlas/3.10.3\n\u251c\u2500\u2500 linux-ubuntu20.04-aarch64\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 linux-ubuntu20.04-ppc64le\n    \u2514\u2500\u2500 ...\n\n</code></pre> <p>The Unity module set is split in two. <code>/modules/modulefiles</code> contains our homebrew modules, those compiled by hand by the admins. <code>/modules/spack_modulefiles</code> contains modules created by Spack. <code>/modules/modulefiles</code> is not changing.</p> <p>At the start of each new login shell, <code>$MODULEPATH</code> contains only <code>/modules/modulefiles/</code> and <code>/modules/spack_modulefiles/linux-ubuntu20.04-&lt;architecture&gt;/Core</code>. The \"Core\" directory contains the modules compiled with Ubuntu's default GNU compiler suite, and without any special providers. Modules made with another compiler, or with special providers, are separated from Core and given their own directory. When you <code>module load</code> a compiler or special provider, its directory is added to <code>$MODULEPATH</code>, and any modules in that directory are now available to use.</p>"},{"location":"updates/hierarchy-change.html#also-included-in-this-change","title":"Also included in this change","text":"<ul> <li>Lmod will no longer allow you to load modules without a version name.</li> <li>Every new Slurm job will <code>module purge</code>. You will have to make sure that your jobs load modules all by themselves.<ul> <li>This is to ensure that incompatible modules are not used.</li> </ul> </li> <li>New commands are available in your shell:<ul> <li><code>unity-module-find</code></li> <li><code>unity-module-hierarchy</code></li> <li><code>unity-module-hierarchy-help</code></li> <li><code>unity-slurm-list-constraints</code></li> <li><code>unity-slurm-find-nodes</code></li> </ul> </li> <li>We are switching Spack TCL modules to Lua.<ul> <li>This shouldn't affect the user experience.</li> </ul> </li> </ul>"},{"location":"updates/hierarchy-change.html#learn-more","title":"Learn more","text":"<p>Using the Module Hierarchy</p> <p>https://lmod.readthedocs.io/en/latest/010_user.html#module-hierarchy</p> <p>https://lmod.readthedocs.io/en/latest/080_hierarchy.html</p>"}]}