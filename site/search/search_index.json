{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Unity This introductory tutorial will help you build an understanding of what an HPC, or high performance computing cluster is, and how to most effectively utilize it. Defining some Terms At its most basic level, you are learning how to use a cluster . A cluster is many servers (or computers) joined together in an effort to work together, where a single server is known as a node . Unity is an HPC , or High Performance Computing cluster. This means we focus most on computational power and efficiency, as the name entails. The primary use case of Unity is by researchers wanting more computational power than what is available on their own. Think about your personal laptop/desktop: when you use your computer, the system decides the resources on your computer to use (cpu, ram, etc.) based on what you are doing at that time. When you scale this process up to a cluster, what is known as a scheduler determines what resources to give you, but this time across many computers, not just one. You can picture the cluster as a scaled-up version of a single personal computer. An operation you run on the cluster is referred to as a job . How Unity Works While you may not need to master every bit of the operation here at the Unity Cluster, it is important that you generally know how the cluster operates, because it can help your troubleshooting in the future. Below is a simplified diagram of the structure of Unity. The process in which you use the cluster: 1. The client connects to Unity using SSH, or the Jupyter portal. 1. Once connected, a job is requested through the scheduler, and your job is placed in the appropriate queue. 1. Once resources are available (cores, gpu, memory, etc.), the scheduler starts your job. 1. Once your job completes, the result returns to the client. The above process can be viewed below: 1. Connecting to the cluster You can connect to Unity in two ways, an SSH connection (the standard linux console), and an instance of JupyterLab. JupyterLab is the easiest to get up and going. When connecting the portal, you will be asked to select one of a few preset resources to allocate for your jupter notebook. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterLab as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible to starting your own jobs. This can be more useful than jupyter for jobs that last a long time that must be left unattended, or to have much more refined control over the resources allocated for your job. Connecting the cluster is discussed in more detail here . 2. Requesting Resources If you are on an SSH connection, you will have to manually request resources. Once you decide on what resources you want, you will submit that information to the scheduler, which will place you in a queue. If your resources are available immediately, your request will return right away, if not, you will be held in the queue until your requested resources become available. Requesting resources in the cluster and all parameters allowed is discussed in more detail here . 3. Starting Job Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client. 4. Ending Job Once the job has finished, the scheduler will return whatever info you requested in your parameters.","title":"Home"},{"location":"#introduction-to-unity","text":"This introductory tutorial will help you build an understanding of what an HPC, or high performance computing cluster is, and how to most effectively utilize it.","title":"Introduction to Unity"},{"location":"#defining-some-terms","text":"At its most basic level, you are learning how to use a cluster . A cluster is many servers (or computers) joined together in an effort to work together, where a single server is known as a node . Unity is an HPC , or High Performance Computing cluster. This means we focus most on computational power and efficiency, as the name entails. The primary use case of Unity is by researchers wanting more computational power than what is available on their own. Think about your personal laptop/desktop: when you use your computer, the system decides the resources on your computer to use (cpu, ram, etc.) based on what you are doing at that time. When you scale this process up to a cluster, what is known as a scheduler determines what resources to give you, but this time across many computers, not just one. You can picture the cluster as a scaled-up version of a single personal computer. An operation you run on the cluster is referred to as a job .","title":"Defining some Terms"},{"location":"#how-unity-works","text":"While you may not need to master every bit of the operation here at the Unity Cluster, it is important that you generally know how the cluster operates, because it can help your troubleshooting in the future. Below is a simplified diagram of the structure of Unity. The process in which you use the cluster: 1. The client connects to Unity using SSH, or the Jupyter portal. 1. Once connected, a job is requested through the scheduler, and your job is placed in the appropriate queue. 1. Once resources are available (cores, gpu, memory, etc.), the scheduler starts your job. 1. Once your job completes, the result returns to the client. The above process can be viewed below:","title":"How Unity Works"},{"location":"#1-connecting-to-the-cluster","text":"You can connect to Unity in two ways, an SSH connection (the standard linux console), and an instance of JupyterLab. JupyterLab is the easiest to get up and going. When connecting the portal, you will be asked to select one of a few preset resources to allocate for your jupter notebook. Once you attempt to spawn your notebook and resources become available, you will be able to use JupyterLab as if it is running on your own computer. SSH is the more traditional method of using an HPC cluster. You will connect to the login node of unity, and you will be responsible to starting your own jobs. This can be more useful than jupyter for jobs that last a long time that must be left unattended, or to have much more refined control over the resources allocated for your job. Connecting the cluster is discussed in more detail here .","title":"1. Connecting to the cluster"},{"location":"#2-requesting-resources","text":"If you are on an SSH connection, you will have to manually request resources. Once you decide on what resources you want, you will submit that information to the scheduler, which will place you in a queue. If your resources are available immediately, your request will return right away, if not, you will be held in the queue until your requested resources become available. Requesting resources in the cluster and all parameters allowed is discussed in more detail here .","title":"2. Requesting Resources"},{"location":"#3-starting-job","text":"Once the scheduler has started your job, it will run on some node in the cluster, using some resources that were defined by your parameters. It is not important what node the job runs on from the point of view of the client.","title":"3. Starting Job"},{"location":"#4-ending-job","text":"Once the job has finished, the scheduler will return whatever info you requested in your parameters.","title":"4. Ending Job"},{"location":"connecting/jupyter/","text":"JupyterHub Portal The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal here and simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"Web - Jupyter Lab"},{"location":"connecting/jupyter/#jupyterhub-portal","text":"The simplest method of using the cluster is using the Jupyter portal. You can access the jupyter portal here and simply select the resources you want and get started right away. Comprehensive documentation for JupyterLab can be found here .","title":"JupyterHub Portal"},{"location":"connecting/ssh/","text":"SSH Connection The most traditional method of connecting to the cluster is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will be using on Unity. SSH stands for \"secure shell\". Configure SSH Keys The authentication method we use to allow users to connect to SSH is using SSH keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key, which is stored on the server, and a private key, which you keep on your local computer. In very basic terms, you authenticate the public key with your private key and that allows you to login to the cluster. You must save your SSH public key on the cluster by adding it in account settings . If you are unsure how to generate a public/private key pair. Simply click on generate key to add the public key. The private key will be downloaded. Connection Details If you know what to do with this information already, you can skip reading everything below that. Address unity.rc.umass.edu Username NETID_school_edu - View your username here If you don't, we will explain how to set it up below. Windows Users Windows users can use PuTTY to connect to the cluster. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to the cluster right away. Mac Users Coming soon...I have to find a mac Linux Users Most distributions of linux come with the openssh client, which you can use to connect to the cluster. If the file ~/.ssh/config doesn't exist, create it. Append the following contents: Host unity HostName unity.rc.umass.edu User <NETID>_umass_edu IdentityFile <PATH_TO_PRIVATE_KEY> You can then connect to the cluster using the command ssh unity .","title":"Console - SSH"},{"location":"connecting/ssh/#ssh-connection","text":"The most traditional method of connecting to the cluster is using an SSH connection. A shell is what you type commands into. The most common shell in linux is bash, which is what you will be using on Unity. SSH stands for \"secure shell\".","title":"SSH Connection"},{"location":"connecting/ssh/#configure-ssh-keys","text":"The authentication method we use to allow users to connect to SSH is using SSH keys. You can read more about the public/private key exchange here . For the purposes of this guide, you should know that there is a public key, which is stored on the server, and a private key, which you keep on your local computer. In very basic terms, you authenticate the public key with your private key and that allows you to login to the cluster. You must save your SSH public key on the cluster by adding it in account settings . If you are unsure how to generate a public/private key pair. Simply click on generate key to add the public key. The private key will be downloaded.","title":"Configure SSH Keys"},{"location":"connecting/ssh/#connection-details","text":"If you know what to do with this information already, you can skip reading everything below that. Address unity.rc.umass.edu Username NETID_school_edu - View your username here If you don't, we will explain how to set it up below.","title":"Connection Details"},{"location":"connecting/ssh/#windows-users","text":"Windows users can use PuTTY to connect to the cluster. Download and install PuTTY by following the link above. Be sure to select the 64 bit / 32 bit download depending on your system. Most are 64 bit, but if you are unsure 32 bit will always work. Open PuTTY and enter hostname unity.rc.umass.edu on the main page On the left sidebar, navigate to connection -> data, and enter your username. On the left sidebar, navigate to connction -> ssh -> auth, and browse to your private key location. Finally, in the main screen again, save the profile you created so you don't have to enter this information every time. Enter unity as the profile name and click save . You can then double click on unity under saved sessions to connect to the cluster right away.","title":"Windows Users"},{"location":"connecting/ssh/#mac-users","text":"Coming soon...I have to find a mac","title":"Mac Users"},{"location":"connecting/ssh/#linux-users","text":"Most distributions of linux come with the openssh client, which you can use to connect to the cluster. If the file ~/.ssh/config doesn't exist, create it. Append the following contents: Host unity HostName unity.rc.umass.edu User <NETID>_umass_edu IdentityFile <PATH_TO_PRIVATE_KEY> You can then connect to the cluster using the command ssh unity .","title":"Linux Users"},{"location":"slurm/","text":"Instruction to Slurm: The Job Scheduler Slurm is the job scheduler we use in our cluster. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user needs to know should be available. Partitions / Queues Our cluster has a number of slurm partitions defined, each of which corresponds to a job queue : * cpu-short * cpu-long * gpu As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. cpu-short has a 4 hour runtime limit to facilitate more efficient queuing for those wishing to run shorter jobs. The other partitions have no runtime limit. To view all available partitions in Slurm, you can run the command sinfo . For those that purchase hardware for Unity and choose to have priority/sole access to a partition, that parition will be listed but if you are not a member of that group you will not be able to use it. Jobs A job is an operation which the user submits to the cluster to run under allocated resources. When submitting a job to the cluster, you will have a few options. The first is to use srun , which will simply run the command you specify in the srun command with any parameters specified. The other option is an interactive job. This job will allocate resources and start the job interactively, so that you can view what is happening live and interact with the prompt. If you leave this prompt the job will be cancelled . For example, you can use this feature together with the bash command to open an interactive bash shell with allocated resources.","title":"Description"},{"location":"slurm/#instruction-to-slurm-the-job-scheduler","text":"Slurm is the job scheduler we use in our cluster. More info about what a job scheduler is can be found in the introduction . Here we will go more into depth about some elements of the scheduler. There are many more features of Slurm that go beyond the scope of this guide, but all that you as a user needs to know should be available.","title":"Instruction to Slurm: The Job Scheduler"},{"location":"slurm/#partitions-queues","text":"Our cluster has a number of slurm partitions defined, each of which corresponds to a job queue : * cpu-short * cpu-long * gpu As you may have guessed, you as the user request to use a specific partition based on what resources your job needs. cpu-short has a 4 hour runtime limit to facilitate more efficient queuing for those wishing to run shorter jobs. The other partitions have no runtime limit. To view all available partitions in Slurm, you can run the command sinfo . For those that purchase hardware for Unity and choose to have priority/sole access to a partition, that parition will be listed but if you are not a member of that group you will not be able to use it.","title":"Partitions / Queues"},{"location":"slurm/#jobs","text":"A job is an operation which the user submits to the cluster to run under allocated resources. When submitting a job to the cluster, you will have a few options. The first is to use srun , which will simply run the command you specify in the srun command with any parameters specified. The other option is an interactive job. This job will allocate resources and start the job interactively, so that you can view what is happening live and interact with the prompt. If you leave this prompt the job will be cancelled . For example, you can use this feature together with the bash command to open an interactive bash shell with allocated resources.","title":"Jobs"},{"location":"slurm/sbatch/","text":"Using SBATCH to Submit Jobs SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue . SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever shell, if you don't know just use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -n 4 # Number of Cores per Node #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"SBATCH Jobs"},{"location":"slurm/sbatch/#using-sbatch-to-submit-jobs","text":"SBATCH is a non-blocking command, meaning there is not a circumstance where running the command will cause it to hold. Even if the resources requested are not available, the job will be thrown into the queue and will start to run once resources become available. The status of a job can be seen using squeue . SBATCH is based around running a single file. That being said, you shouldn't need to specify any parameters in the command other than sbatch <batch file> , because you can specify all parameters in the command inside the file itself. The following is an example of a batch script. Please note that the top of the script must start with #!/bin/bash (or whatever shell, if you don't know just use bash), and then immediately follow with #SBATCH <param> parameters. An example of common SBATCH parameters and a simple script is below, this script will allocate 4 CPUs and one GPU in the GPU partition. #!/bin/bash #SBATCH -n 4 # Number of Cores per Node #SBATCH --mem=8192 # Requested Memory #SBATCH -p gpu # Partition #SBATCH -G 1 # Number of GPUs #SBATCH -t 01:00:00 # Job time limit #SBATCH -o slurm-%j.out # %j = job ID module load cuda /modules/apps/cuda/10.1.243/samples/bin/x86_64/linux/release/deviceQuery This script should query the available GPUs, and print only one device to the specified file. Feel free to remove/modify any of the parameters in the script to suit your needs.","title":"Using SBATCH to Submit Jobs"},{"location":"slurm/srun/","text":"Using SRUN to Submit Jobs Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job -G <num> Number of GPUs to allocate to the job --mem <num> Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu-short --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu-short --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"SRUN Jobs"},{"location":"slurm/srun/#using-srun-to-submit-jobs","text":"Note Usually, if you have to run a single application multiple times, or if you are trying to run a non-interactive application, you should use sbatch instead of srun, since sbatch allows you to specify parameters in the file, and is non-blocking (see below). SRUN is a so-called blocking command, as in it will not let you execute other commands until this command is finished (not necessarily the job, just the allocation). For example, if you run srun /bin/hostname and resources are available right away, the job will be sent out and the result saved into a file. If resources are not available, you will be stuck in the command while you are pending in the queue. Please note that like sbatch, you can run a batch file using srun. The command syntax is srun <options> [executable] <args> Options is where you can specify the resources you want for the executable, or define. The following are some of the options available; to see all available parameters run man srun . -c <num> Number of CPUs (threads) to allocate to the job -G <num> Number of GPUs to allocate to the job --mem <num> Memory to allocate to the job (in MB by default) -p <partition> Partition to submit the job to To run an interacitve job (in this case a bash prompt), the command might look like this ( --pty is the important option): srun -c 6 -p cpu-short --pty bash To run an application on the cluster that uses a GUI, you must use an interactive job, in addition to the --x11 argument: srun -c 6 -p cpu-short --pty --x11 xclock Note You cannot run an interactive/gui job using the sbatch command, you must use srun .","title":"Using SRUN to Submit Jobs"}]}